colour = Class
)  +
ggforce::facet_row(
facets = ~Class,
scales = "fixed",
space = "fixed",
drop = TRUE
) +
theme(legend.position = "none")
return(plot)
}
# bind rows to create a single dataframe for each object, then bind them all together
(perceived_objective_regressions = rbind(dplyr::bind_rows(abundance_uv_reg_results), dplyr::bind_rows(visibility_uv_reg_results), dplyr::bind_rows(access_uv_reg_results), dplyr::bind_rows(usage_uv_reg_results), dplyr::bind_rows(quality_uv_reg_results),dplyr::bind_rows(abundance_uv_perc_reg_results), dplyr::bind_rows(visibility_uv_perc_reg_results), dplyr::bind_rows(access_uv_perc_reg_results), dplyr::bind_rows(usage_uv_perc_reg_results), dplyr::bind_rows(quality_uv_perc_reg_results)))
# cleaning to make the forest plot -- naming, factorizing
(perceived_objective_regressions = perceived_objective_regressions %>%
# factorize and rename greenspace levels for ggplot
mutate(`Greenspace type` = factor(dependent_var,
levels = c("grn_abundance_binary","grn_visibility_binary","grn_access_binary","grn_usage_binary","grn_quality_binary"),
labels = c("Abundance","Visibility","Access","Usage","Quality"))) %>%
mutate(index = factor(index,
levels = c("scaled_MN19_300","scaled_MN19_500","scaled_MN19_1000","scaled_MN19_N","scaled_ML19_300","scaled_ML19_500","scaled_ML19_1000","scaled_ML19_N","scaled_DRCOG_300","scaled_DRCOG_500","scaled_DRCOG_1000","scaled_DRCOG_N","scaled_gvi_300","scaled_gvi_500","scaled_gvi_1000","scaled_gvi_N"),
labels = c("NAIP NDVI - 300 m","NAIP NDVI - 500 m","NAIP NDVI - 1000 m","NAIP NDVI - self-drawn","Landsat NDVI - 300 m","Landsat NDVI - 500 m","Landsat NDVI - 1000 m","Landsat NDVI - self-drawn","Pct. vegetation - 300 m", "Pct. vegetation - 500 m", "Pct. vegetation - 1000 m", "Pct. vegetation - self-drawn", "GVI - 300 m", "GVI - 500 m", "GVI - 1000 m", "GVI - self-drawn"))))
# change order
perceived_objective_regressions = perceived_objective_regressions[order(perceived_objective_regressions$dependent_var,perceived_objective_regressions$index), ]
# plot -- NOT using formula due to difference in formula structure (no class, no facets)
plot = ggforestplot::forestplot(
df = perceived_objective_regressions,
name = index,
estimate = Estimate,
se = c(`Std. Error`),
logodds = TRUE,
pvalue = adj_p,
psignif = 0.05,
shape = `P-value`,
colour = `Greenspace type`,
xlab = "Odds ratio for greenspace perception (95% CI) per 1-SD increment in objective greenspace exposure",
ylab = "Objective greenspace type and buffer size"
)
plot
# Univariate forest plots
# for unadjusted and adjusted models combine results from each class (1-3) into single DF, clean names for plotting
(unadjusted_class_regressions = clean_names(rbind(dplyr::bind_rows(unadjusted_class1_results), dplyr::bind_rows(unadjusted_class2_results), dplyr::bind_rows(unadjusted_class3_results))))
(adjusted_class_regressions = clean_names(rbind(dplyr::bind_rows(adjusted_class1_results), dplyr::bind_rows(adjusted_class2_results), dplyr::bind_rows(adjusted_class3_results))))
(unadjusted_uv_forest_plot = forest_plot(unadjusted_class_regressions))
(adjusted_uv_forest_plot = forest_plot(adjusted_class_regressions %>% filter(!index %in% c("NAIP NDVI (300 m)"))))
# Multivariate forest plots
#(unadjusted_multi_class_regressions = clean_names(rbind(dplyr::bind_rows(class1_multi_results_unadjusted), dplyr::bind_rows(class2_multi_results_unadjusted), dplyr::bind_rows(class3_multi_results_unadjusted))))
(adjusted_multi_class_regressions = clean_names(rbind(dplyr::bind_rows(class1_multi_results_adjusted), dplyr::bind_rows(class2_multi_results_adjusted), dplyr::bind_rows(class3_multi_results_adjusted))))
#(unadjusted_multi_forest_plot = forest_plot(unadjusted_multi_class_regressions))
(adjusted_multi_forest_plot = forest_plot(adjusted_multi_class_regressions))
library(sf)
library(tmap)
library(terra)
# read in denver neighborhodds, change coordinate system
denver = st_read("/Users/esrieves/Documents/GitHub/thesis/LCA_grn/Denver_neighborhoods")
st_crs(denver) = 4326
denver
# add neighborhood income - greenspace classification
high_inc_low_grn = c("Union Station","Central Business District","Highland","North Capitol Hill")
high_inc_high_grn = c("Country Club","Hilltop","South Park Hill")
low_inc_high_grn = c("Barnum","Montbello","Windsor","East Colfax")
low_inc_low_grn = c("Lincoln Park","Valverde","Elyria Swansea","Globeville")
## implement in dataframe
(denver = denver %>%
mutate(grn_inc_class = ifelse(NBHD_NAME %in% high_inc_high_grn,"High Income, High Greenspace",
ifelse(NBHD_NAME %in% low_inc_low_grn,"Low Income, Low Greenspace",
ifelse(NBHD_NAME %in% low_inc_high_grn,"Low Income, High Greenspace",
ifelse(NBHD_NAME %in% high_inc_low_grn,"High Income, Low Greenspace","Neighborhood Not Targeted"))))))
# make into a factor
denver$grn_inc_class=factor(denver$grn_inc_class)
# denver %>% filter(NBHD_NAME %in% c("Union Station","Central Business District","Highland","North Capitol Hill", "Country Club","Hilltop","South Park Hill", "Barnum","Montbello","Windsor","East Colfax", "Lincoln Park","Valverde","Elyria Swansea","Globeville"))
# make participants a spatial object to plot
participants_spatial = st_as_sf(all_data, coords=c("X", "Y"), crs=4326)
# make participants a spatial object to plot
all_data_LCA
# make participants a spatial object to plot
all_data_LCA  %>% na.omit()
participants_spatial = st_as_sf(all_data, coords=c("X", "Y"), crs=4326)
# make participants a spatial object to plot
all_data_LCA
# make participants a spatial object to plot
all_data %>% na.omit()
# make participants a spatial object to plot
all_data
# make participants a spatial object to plot
all_data %>% na.omit()
# make participants a spatial object to plot
all_data
# make participants a spatial object to plot
all_data %>% drop_na(c("X","Y"))
# make participants a spatial object to plot
map = all_data %>% drop_na(c("X","Y"))
participants_spatial = st_as_sf(map, coords=c("X", "Y"), crs=4326)
# plot typologies w tmap
tm_shape(denver) +
tm_polygons(col = "grn_inc_class",
palette = c("darkolivegreen3","darksalmon","darkseagreen3","deeppink3","gray"),
title = "Neighborhood classification by greenspace and income") +
tm_shape(participants_spatial) +
# jitter makes it easier to see clusters where there's a single dot
tm_dots(jitter=0.1) +
tm_layout(frame = FALSE)
# plot typologies w tmap
tm_shape(denver) +
tm_polygons(col = "grn_inc_class",
palette = c("darkolivegreen3","darksalmon","darkseagreen3","deeppink3","light gray"),
title = "Neighborhood classification by greenspace and income") +
tm_shape(participants_spatial) +
# jitter makes it easier to see clusters where there's a single dot
tm_dots(jitter=0.1) +
tm_layout(frame = FALSE)
# plot typologies w tmap
tm_shape(denver) +
tm_polygons(col = "grn_inc_class",
palette = c("darkolivegreen3","darksalmon","darkseagreen3","deeppink3","gray"),
title = "Neighborhood classification by greenspace and income") +
tm_shape(participants_spatial) +
# jitter makes it easier to see clusters where there's a single dot
tm_dots(jitter=0.1) +
tm_layout(frame = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(sp)
library(gstat)
library(raster)
library(tidyverse)
library(spacetime)
library(stringr)
library(readr)
library(leaflet)
library(gridExtra)
library(gifski)
library(nngeo)
# to make the spherical geometry errors go away
sf::sf_use_s2(FALSE)
# AIR QUALITY
## corrected air quality
AQ_df = read.csv("../intermediary_outputs/corrected_AQ_data.csv")
## sensor info
sensors = read.csv("../intermediary_outputs/sensor_data_full.csv")
# BOUNDARIES
# Boulder boundaries
BO_CO = st_read("../GIS_inputs_destruction_fireboundary/Boulder_county_munis/Municipalities.shp")
fire_counties = BO_CO %>%
dplyr::filter(ZONEDESC == "Louisville" |
ZONEDESC == "Superior" |
ZONEDESC == "Broomfield" |
ZONEDESC == "Lafayette" |
ZONEDESC == "Boulder")
# Boulder Boundary
boulder_precincts = st_read("../GIS_inputs_destruction_fireboundary/Unincorporated_Boulder/Unincorporated_Boulder.shp")
# Broomfield Boundary
broomfield_precincts = st_read("../GIS_inputs_destruction_fireboundary/Broomfield_Precincts/Precincts.shp")
# Westminster Boundary
westminster_city = st_read("../GIS_inputs_destruction_fireboundary/Westminster_CityLimits/CityLimits.shp")
# prg - set projection for everything
prg = "+proj=utm +zone=13 +datum=WGS84 +units=km +no_defs"
# make all the precincts have the same columns (we don't need a lot of the data)
boulder_precincts <- boulder_precincts %>%
select(c(OBJECTID, geometry, SHAPEarea, SHAPElen))
broomfield_precincts <- broomfield_precincts %>%
mutate(SHAPEarea = Shape__Are,
SHAPElen = Shape__Len,
OBJECTID = OBJECTID + 1000) %>% # add 1,000 to the broomfield precincts to get rid of overlapping labels
select(-c("GlobalID", "GIS_ID", "PRECINCT_N", "MAP_COLOR", "USER_COMME", "LAST_UPDAT", "Shape__Are", "Shape__Len"))
westminster_city <- westminster_city %>%
mutate(SHAPEarea = ShapeSTAre,
SHAPElen = ShapeSTLen) %>%
select(c(OBJECTID, geometry, SHAPEarea, SHAPElen))
# save proj string for fire-affected counties to use for other transformations
#prg = raster::crs(fire_counties,asText=TRUE)
# all boundaries joined together
all_bounds <- st_transform(boulder_precincts, crs = prg) %>%
rbind(st_transform(broomfield_precincts, crs=prg)) %>%
rbind(st_transform(westminster_city, crs=prg))
# remove the holes that existed so we just have the outside boundary of this area
all_bounds <- all_bounds %>%
st_union() %>%
st_remove_holes()
# Marshall fire boundary
wfigs_fire = st_read("../GIS_inputs_destruction_fireboundary/WFIGS_-_Wildland_Fire_Perimeters_Full_History/FH_Perimeter.shp")
# filter fire to marshall fire only; update crs
marshall_fire = wfigs_fire %>% filter(poly_Incid == "Marshall") %>% st_transform(crs = st_crs(prg))
# BUILDINGS
# Read in destroyed home and building sites
destroyed_homes = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_homes.shp")
damaged_homes = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/damaged_homes.shp")
destroyed_businesses = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_businesses.shp")
damaged_businesses = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/damaged_businesses.shp")
# create 'type' column; select only the columns needed; make projection string match
destroyed_homes = destroyed_homes %>%
mutate(type = "destroyed - residential") %>%
dplyr::rename(jurisdiction = JURISDICTI) %>%
dplyr::select(jurisdiction, type, latlong, geometry) %>%
st_transform(crs = st_crs(prg))
destroyed_businesses = destroyed_businesses %>%
mutate(type = "destroyed - non-residential") %>%
dplyr::rename(jurisdiction = Jurisdicti) %>%
dplyr::select(jurisdiction, type, latlong, geometry) %>%
st_transform(crs = st_crs(prg))
damaged_homes = damaged_homes %>%
mutate(type = "damaged - residential") %>%
dplyr::rename(jurisdiction = JURISDICTI) %>%
dplyr::select(jurisdiction, type, latlong, geometry) %>%
st_transform(crs = st_crs(prg))
damaged_business = damaged_businesses %>%
mutate(type = "damaged - non-residential") %>%
dplyr::rename(jurisdiction = Jurisdicti) %>%
dplyr::select(jurisdiction, type, latlong, geometry) %>%
st_transform(crs = st_crs(prg))
# combine into one df, using the "," indicator keeps decimal points
destroyed_damaged = rbind(destroyed_homes,
destroyed_businesses,
damaged_homes,
damaged_business) %>%
separate(latlong, c("lat","long"), ",", convert = FALSE)
# change lat and long to floats
destroyed_damaged$lat = as.double(destroyed_damaged$lat)
destroyed_damaged$long = as.double(destroyed_damaged$long)
# create "location status" column for mapping]
sensors$loc_status = str_c(sensors$Location, " - ", sensors$Status)
# make sure we only have valid sensors that have data
sensors = na.omit(sensors)
# set factors to false
options(stringsAsFactors = FALSE)
# create palette for destroyed and damaged building colors for leaflet map
pal = colorFactor(c("red","orange","magenta","light pink"), domain = unique(destroyed_damaged$type))
## create indoor/outdoor icons
sensorIcon = awesomeIconList(
"inside" = makeAwesomeIcon(
icon = "home",
iconColor = "white",
markerColor = "blue",
library = "fa"
),
"outside" = makeAwesomeIcon(
icon = "tree",
iconColor = "white",
markerColor = "green",
library = "fa"
))
sensors
# interactive map of damaged/destroyed buildings and AQ sensors (indoor/outdoor)
# chose not to do icons for destroyed/damaged buildings because the map got really busy
fire_destruction_and_AQ_plot = leaflet(sensors) %>%
addTiles() %>%
addAwesomeMarkers(icon = ~sensorIcon[Location],
lng = sensors$Lon, lat = sensors$Lat) %>%
addCircleMarkers(color = ~pal(destroyed_damaged$type),
radius = 3.5,
opacity = 1,
lng = destroyed_damaged$long, lat = destroyed_damaged$lat) %>%
addLegend("topright", pal = pal, values = ~destroyed_damaged$type,
title = "Fire damage")
# create palette for location + time period
timeSensorIcon = awesomeIconList(
"inside - Complete data throughout fire period" = makeAwesomeIcon(
icon = "home",
iconColor = "white",
markerColor = "green",
library = "fa"
),
"inside - Sensor offline during fire, did not return online" = makeAwesomeIcon(
icon = "tree",
iconColor = "white",
markerColor = "gray",
library = "fa"
),
"inside - Sensor offline during fire, returned online" = makeAwesomeIcon(
icon = "home",
iconColor = "white",
markerColor = "blue",
library = "fa"
),
"outside - Complete data throughout fire period" = makeAwesomeIcon(
icon = "home",
iconColor = "white",
markerColor = "green",
library = "fa"
),
"outside - Sensor offline during fire, did not return online" = makeAwesomeIcon(
icon = "tree",
iconColor = "white",
markerColor = "gray",
library = "fa"
),
"outside - Sensor offline during fire, returned online" = makeAwesomeIcon(
icon = "tree",
iconColor = "white",
markerColor = "blue",
library = "fa"
))
# map based on when sensor was added
(fire_destruction_and_AQ_plot = leaflet(sensors) %>%
addTiles() %>%
addAwesomeMarkers(icon = ~timeSensorIcon[loc_status],
lng = sensors$Lon, lat = sensors$Lat) %>%
addCircleMarkers(color = ~pal(destroyed_damaged$type),
radius = 3.5,
opacity = 1,
lng = destroyed_damaged$long, lat = destroyed_damaged$lat) %>%
addLegend("topright", pal = pal, values = ~destroyed_damaged$type,
title = "Fire damage") %>%
addLegend("bottomright",
pal = colorFactor(c("green","gray","blue"), domain = unique(sensors$Status)),
values = ~sensors$Status, title = "Sensor Time Info"))
grd <- SpatialPixels(SpatialPoints(as_Spatial(st_make_grid(all_bounds, n=50))), proj4string = prg)
plot(grd)
grd <- grd[as_Spatial(all_bounds),]
plot(grd)
grd <- spTransform(grd, CRS(prg))
library(lubridate)
# Cleaning to prepare to make spatial objects
# first, re-format datetime as a xts object
AQ_df$datetime = as.POSIXct(AQ_df$datetime)
# filter out extreme values from Eisenhower dr PA sensor
AQ_df%>% filter(ID == 91877 & corrected_pm > 1000) # 319 over 1000
AQ_df[AQ_df$ID == 91877 & AQ_df$corrected_pm > 1000, ] = NA
AQ_df %>% filter(ID == 91877 & corrected_pm > 1000) # 0 obs over 1000
# use floor date (lubridate) to effectively resample into daily posixct object, then calculate daily average for each ID
daily_AQ_df = AQ_df %>% mutate(day = floor_date(datetime, "day")) %>% group_by(ID,day) %>% summarise(daily_mean = mean(corrected_pm)) # 1622 distinct rows
# create dataframe of sensor data to merge with daily data
sensor_data_df = AQ_df %>% select(ID, Name, Lon, Lat, Location)
sensor_data_df = sensor_data_df %>% na.omit() %>% distinct() # 53 distinct sensors (one row of NAs)
# merge daily AQ df and sensor data df, distinct removes any duplicates that would arise
AQ_data = merge(sensor_data_df,daily_AQ_df,by="ID") # should be 1621 rows
AQ_data
# check for NAs -- not all sensors have all data, 42 sensors that don't have data (but more that are just excluded in the index)
sapply(AQ_data, function(x) sum(is.na(x)))
# cleaning -- remove wonky sensors
AQ_data = AQ_data %>%
# already filtered out Eisenhower Dr outliers (PM > 1000) above before aggregation
# exclude any non-outside sensors
filter(Location == "outside") %>%
# drop sensor 112606 (Wildwood Ct) because it has 0% confidence according to PurpleAir map (per Zac)
filter(ID != 112606)
# determine the number of sensors for days < Feb 4 -- looks like there are 48 overall with 30 sensors during various phases of the time period (1/8 - 2/4)
AQ_data %>% group_by(ID) %>% summarise(count = n())
AQ_data %>% group_by(ID) %>% filter(day < "2022-02-05") %>% summarise(count = n())
# determine which IDs have enough data within the study period to be used to create the monthly average variable
study_completeness = AQ_data %>%
# filter to include only study period
filter(day >= "2022-01-08" & day < "2022-02-05") %>%
group_by(ID) %>%
# create completeness variable, need 70% of days in period to be complete
# 28 days in study period, use this to determine complete pct
summarise(day_count = n(),
complete_pct = (day_count/28) * 100,
is_complete = ifelse(complete_pct >= 70, "complete", "incomplete")) # 8 sensors are complete
# merge with completeness variable with AQ_data to create AQ_data_study_period
(AQ_data_study_period = merge(study_completeness %>% select(ID,is_complete),AQ_data, by = "ID")) # only 1174 rows, probably because of removed dates
# these seem really low...
AQ_data_study_period = AQ_data_study_period %>%
# filter to include only the study period
filter(day >= "2022-01-08" & day < "2022-02-05") %>%
# filter to only include complete sensors
filter(is_complete == "complete") %>%
# groupby ID to calculate the "study mean" -- the mean over the study time period
group_by(ID) %>%
summarise(study_mean = mean(daily_mean),
Lat = Lat,
Lon = Lon) %>%
# keep distinct observations
distinct()
AQ_data_study_period
# to gut check
AQ_data %>% filter(ID == 65669)
AQ_data_study_period
AQ_data_study_period %>% gt()
library(gt)
AQ_data_study_period %>% gt()
AQ_data_study_period
# create spatial points object for each sensor
PA_sensors = SpatialPoints(AQ_data[!duplicated(AQ_data$ID), c("Lon", "Lat")],proj4string = CRS(prg))
summary(PA_sensors) # 48 sensors
# construct spatiotemporal object
PA_STFDF = stConstruct(AQ_data, space = c("Lon","Lat"), time = "day", crs = CRS(prg), SpatialObj = PA_sensors)
# turn ST object into a STFDF (stationary points)
PA_STFDF = as(PA_STFDF,"STFDF")
# see summary of data
summary(PA_STFDF)
# object class
class(PA_STFDF)
# object dimensions
dim(PA_STFDF)
# not all sensors have complete data
study_period = AQ_data %>%
# filter to correct dates
filter(day >= "2022-01-08" & day < "2022-02-05") %>%
# no data for this day
filter(ID != 119691)
# max value during the time period
max(study_period$daily_mean, na.rm = TRUE)
# create plot -- png makes blurry I recommend screenshot
#png("../images/manuscript/time_series.png",width = 850, height = 600)
print(ggplot(data = study_period, aes(x = day, y = daily_mean)) +
geom_line() +
facet_wrap(~ID) +
labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
title = expression("Time series of daily average PM"[2.5]*" for each sensor from January 8 to February 4, 2022")))
#dev.off()
#make id a char for grouping
study_period$ID = as.character(study_period$ID)
print(ggplot(data = study_period, aes(x = day, y = daily_mean, color=ID)) +
geom_line() +
labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
title = expression("Time series of daily average PM"[2.5]*" for each sensor from January 8 to February 4, 2022")))
# determine AQ max and min values to create upper and lower bounds for mapping -- use PAstfdf so mapping is consistent across all maps
(aq_max = max(PA_STFDF[,"2022-01-01::2022-02-04"]@data$daily_mean, na.rm = TRUE)) # 172.2092
(aq_min = min(PA_STFDF[,"2022-01-01::2022-02-04"]@data$daily_mean, na.rm = TRUE)) # 0.12695
summary(PA_STFDF[,"2022-01-08::2022-02-04"])
# create equal interval breaks based on aq min and max
(breaks2 = seq(from=aq_min, to=aq_max, length.out = 6))
# helper function to plot
plot_idw_raster_daily <- function(idw, title) {
r <- raster::rasterize(idw, r_grd, field="var1.pred")
extent(r) <- st_bbox(all_bounds)
r_df <- r %>%
rasterToPoints() %>%
data.frame() %>%
mutate(cuts = cut(layer, breaks=breaks2, na.omit=T))
return(ggplot() +
geom_tile(data=r_df, aes(x=x, y=y, fill=cuts))+
scale_fill_brewer(palette = "YlOrRd", name=expression("PM"[2.5]*" (µg/m"^3*")"), drop=FALSE) +
geom_sf(data=marshall_fire$geometry, col="gray20", alpha=0.1) +
geom_point(data = r_df[!is.na(r_df$daily_mean), ], aes(x = x, y = y)) +
labs(y = "Latitude", x = "Longitude",
caption = "Border represents Marshall Fire perimeter") +
ggtitle(title))
}
# jan 1 - feb 4th inclusive
for (j in 1:2) {
date <- paste("2022-0", j, sep="")
if (j == 1) {
range = 1:31
} else {
range = 1:4
}
for (i in range) {
if (i < 10) {
i <- paste("0", i, sep="")
}
day1 <- paste(date, "-", i, sep="")
#print(day1)
daily_data <- PA_STFDF[, day1]
print(daily_data@data)
#print(data@data)
N = n_distinct(daily_data@data$ID)
idw <- idw(daily_mean~1, daily_data[!is.na(daily_data$daily_mean),], grd, idp = 3)
#idw <- idw(daily_mean~1, data, grd, idp = 3)
#png(paste("../images/jan1-feb4_V2/", day1, ".png", sep=''))
print(plot_idw_raster_daily(idw, paste(day1, "IDW, N = ", N)))
#dev.off()
}
}
(monthly_avg_data = AQ_data %>%
filter(day >= "2022-01-08" & day < "2022-02-05") %>%
group_by(ID) %>%
# use 28 in calculating the percent because 28 days in the period
dplyr::summarise(N = n(),
# include Lat and Lon to reduce need for joins
Lat = Lat,
Lon = Lon,
pct_complete = (N/28) * 100,
is_usable = ifelse(pct_complete >= 70, "usable", "not usable"),
mean_pm = mean(daily_mean, na.rm=TRUE)))
(monthly_avg_data = monthly_avg_data %>% filter(is_usable == "usable") %>% distinct() %>% as.data.frame())
row.names(monthly_avg_data) = monthly_avg_data$ID
# make spatial object of monthly average
# start by creating spatial stations
coords = cbind(monthly_avg_data$Lon, monthly_avg_data$Lat)
row.names(coords) = monthly_avg_data$ID
# create spatial data frame of points
(monthly_avg_data_spatial = SpatialPointsDataFrame(coords, monthly_avg_data, proj4string = CRS(prg), match.ID = TRUE))
monthly_avg_data_spatial@data
monthly_idw = idw(mean_pm~1, monthly_avg_data_spatial, grd, idp = 2)
png("../images/manuscript/period_average.png", width = 700, height = 350)
(monthly_idw_fig = plot_idw_raster_daily(idw = monthly_idw, title = expression("Inverse distance weighted interpolated PM"[2.5]*" concentrations averaged from January 8 to February 4, 2022, N = 9")))
dev.off()
sensors_in_monthly_avg = study_period %>%
filter(ID %in% c("65669","68375","81149","85999","86477","91877","92321","136514","137686"))
write.csv(sensors_in_monthly_avg,"../intermediary_outputs/sensors_in_monthly_data.csv")
print(ggplot(data = sensors_in_monthly_avg, aes(x = day, y = daily_mean)) +
geom_line() +
facet_wrap(~ID) +
labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
title = expression("Time series of daily average PM"[2.5]*" for sensors used to create average from January 8 to February 4, 2022")))
#make ID a char for goruping in fig
sensors_in_monthly_avg$ID = as.character(sensors_in_monthly_avg$ID)
print(ggplot(data = sensors_in_monthly_avg, aes(x = day, y = daily_mean, color=ID)) +
geom_line() +
labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
title = expression("Time series of daily average PM"[2.5]*" for sensors used to create average from January 8 to February 4, 2022")))
# using the weird numbers to confirm that this works without corrupting original version.. go back and change everything to this if need be
allbounds3 = all_bounds
allbounds3 = st_transform(allbounds3, crs=4326)
marshall2 = marshall_fire
marshall2 = st_transform(marshall2, crs = 4326)
(basemap = ggplot() +
geom_sf(data = allbounds3) +
geom_sf(data = marshall2))
basemap +
geom_point(
data = monthly_avg_data,
aes(x=Lon,y=Lat,color=mean_pm)
)
ggplot(all_bounds, aes(geometry=geometry)) +
geom_sf() +
geom_sf(data=marshall_fire$geometry, fill="red", alpha=0.6) +
geom_sf(data=st_as_sf(PA_sensors))
## Time series plots
stplot(PA_STFDF[,"2021-12-30::2021-12-31","temp"],mode="ts")
