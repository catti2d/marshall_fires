id_key %>% rowwise() %>% mutate(city = get_sensor_info(ID))
id_key
id_key$city = toString(lapply(id_key$ID, get_sensor_info))
id_key
id_key %>% rowwise() %>% mutate(city = get_sensor_info())
id_key %>% rowwise() %>% mutate(city = get_sensor_info(ID))
id_key
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
info <- info %>%
mutate(city = ifelse(length(city) == 0, NA, city)) %>%
dplyr::select(-Name)
return(info)
}
(info = id_key[id_key$ID == 65669, ])
# create a key coordinating the sensor ID number to its lat/lon
(id_key = AQ_df %>%
group_by(ID) %>%
dplyr::select(ID, Lon, Lat, Name, Location) %>%
unique() %>%
na.omit()
)
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
t
t[1]
t[[1]]
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
# info <- info %>%
#   mutate(city = ifelse(length(city) == 0, NA, city)) %>%
#   dplyr::select(-Name)
return(info)
}
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
# info <- info %>%
#   mutate(city = ifelse(length(city) == 0, NA, city)) %>%
#   dplyr::select(-Name)
return(city)
}
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
id_key$city = toString(lapply(id_key$ID, get_sensor_info))
id_key
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
info = info %>%
mutate(city = ifelse(length(city) == 0, NA, city)) %>%
dplyr::select(-Name)
return(info)
}
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
info = info %>%
mutate(city = ifelse(length(city) == 0, NA, city)) %>%
dplyr::select(-Name)
return(info)
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
(id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID)))
(id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID)))
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
info = info %>%
mutate(city = ifelse(length(city) == 0, NA, city)) %>%
dplyr::select(-Name)
return(info)
}
(id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID)))
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
# select only the city name & return it
city = city_intersect$ZONEDESC
return(city)
}
(id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID)))
id_key %>% rowwise() %>% mutate(t = get_sensor_info(ID))
## Combine lists of sensor data into a single dataframe
# combine all AQ lists from each directory into single AQ dataframe for each directory
(AQ_df1 = rbind.fill(AQ_files1) %>% as.data.frame())
(AQ_df2 = rbind.fill(AQ_files2) %>% as.data.frame())
# combine marshall fire and superior/lousiville AQ sites into a single df
(AQ_df = rbind(AQ_df1, AQ_df2))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(plyr)
library(sf)
library(raster)
library(rgdal)
library(ggplot2)
library(spacetime)
library(gridExtra)
library(stringr)
library(leaflet)
# PM2.5 package recommended by Priyanka
library(bjzresc)
# to make the spherical geometry errors go away
sf::sf_use_s2(FALSE)
## use bjzresc package to get list of purple air sensors; save to df instead of csv
pa_download = getPurpleairLst(output.path = NULL)
## Use marshall fire boundary to extract stations
wfigs_fire = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/WFIGS_-_Wildland_Fire_Perimeters_Full_History/FH_Perimeter.shp")
(marshall_fire = wfigs_fire %>% filter(poly_Incid == "Marshall"))
(prg = crs(marshall_fire,asText=TRUE))
## Use boulder county shapefile to extract municipal boundaries for Louisville and Superior
BO_CO = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/Boulder_county_munis/Municipalities.shp")
(fire_counties = BO_CO %>% filter(ZONEDESC == "Louisville" | ZONEDESC == "Superior" | ZONEDESC == "Broomfield" | ZONEDESC == "Lafayette"))
# same as marshall fire
(crs(fire_counties,asText=TRUE))
## Read in destroyed home and building sites
destroyed_homes = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_homes.shp")
damaged_homes = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/damaged_homes.shp")
destroyed_businesses = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_businesses.shp")
damaged_businesses = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/damaged_businesses.shp")
# check that crs matches leaflet
(crs(destroyed_homes))
# mutate type and select columns needed
(destroyed_homes = destroyed_homes %>% mutate(type = "destroyed - residential") %>% dplyr::rename(jurisdiction = JURISDICTI) %>% dplyr::select(jurisdiction, type, latlong, geometry))
(destroyed_businesses = destroyed_businesses  %>% mutate(type = "destroyed - non-residential") %>% dplyr::rename(jurisdiction = Jurisdicti) %>% dplyr::select(jurisdiction, type, latlong, geometry))
(damaged_homes = damaged_homes %>% mutate(type = "damaged - residential") %>% dplyr::rename(jurisdiction = JURISDICTI) %>% dplyr::select(jurisdiction, type, latlong, geometry))
(damaged_business = damaged_businesses %>% mutate(type = "damaged - non-residential") %>% dplyr::rename(jurisdiction = Jurisdicti) %>% dplyr::select(jurisdiction, type, latlong, geometry))
# combine into one df, using the "," indicator keeps decimal points
(destroyed_damaged =rbind(destroyed_homes,destroyed_businesses,damaged_homes,damaged_business) %>% separate(latlong, c("lat","long"), ",", convert = FALSE))
# change lat and long to floats
destroyed_damaged$lat = as.double(destroyed_damaged$lat)
destroyed_damaged$long = as.double(destroyed_damaged$long)
## Intersect municipal boundaries with PA sensors
# remove null Lat/Long PA sensors -- important to creating spatial dataframe
pa_download = pa_download[complete.cases(pa_download[c("Lat","Lon")]),]
# check that it worked
sum(is.na(pa_download[c("Lat","Lon")]))
# create spatial dataframe
pa_download_spatial = pa_download %>%
st_as_sf(coords = c("Lon","Lat")) %>%
st_set_crs(prg)
# check CRS
crs(pa_download_spatial)
# intersect PA download area and fire affected area to download sensors
fire_affected_sensors = st_intersection(pa_download_spatial,st_buffer(fire_counties,0))
fire_affected_sensors2 = st_intersection(pa_download_spatial,st_buffer(marshall_fire,0))
# get sensor IDs for sensors in fire affected area
fire_area_sensor_IDs = fire_affected_sensors$ID
fire_area_sensor_IDs2 = fire_affected_sensors2$ID
# filter original dataframe to include only sensors in fire affected area
(fire_area_sensors = pa_download[pa_download$ID %in% fire_area_sensor_IDs, ])
(fire_area_sensors2 = pa_download[pa_download$ID %in% fire_area_sensor_IDs2, ])
purpleairDownload(site.csv = fire_area_sensors2, start.date = "2021-12-30", end.date = "2022-03-01", output.path = "marshall_fire_path_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
# function that will get information from a sensor, based on its ID (e.g. 92321)
# a dataframe is returned with 5 columns: the ID number, the lon & lat coordinates, inside/outside location, and superior/louisville city
get_sensor_info <- function(ID) {
info <- id_key[id_key$ID == ID,]
city_pt <- st_intersection(sensor_map$data, st_as_sf(info, coords=c("Lon", "Lat"), crs=4326))["ZONEDESC"]
city <- city_pt$ZONEDESC
# print(city)
info <- info %>%
mutate(city = ifelse(length(city) == 0, NA, city)) %>%
dplyr::select(-Name)
return(info)
}
## Read downloaded PA files from their filepath and turn them into a DF
# directory where files are stored
dir = "fire_counties_PAs/"
dir2 = "marshall_fire_path_PAs/"
# create a list of all file names in this directory
file_name1 = list.files(path=dir, pattern="*.csv", full.names=TRUE)
file_names2 = list.files(path=dir2, pattern="*.csv", full.names=TRUE)
# read csvs for each filename in list --> results in a list of lists
AQ_files1 = lapply(file_name1, read_csv)
AQ_files2 = lapply(file_names2, read_csv)
## Combine lists of sensor data into a single dataframe
# combine all AQ lists from each directory into single AQ dataframe for each directory
(AQ_df1 = rbind.fill(AQ_files1) %>% as.data.frame())
(AQ_df2 = rbind.fill(AQ_files2) %>% as.data.frame())
# combine marshall fire and superior/lousiville AQ sites into a single df
(AQ_df = rbind(AQ_df1, AQ_df2))
# create a date column in POSIX format to create time series
AQ_df$datetime = as.POSIXct(AQ_df$created_at)
# rename and select important columns
AQ_df = AQ_df %>%
dplyr::rename(pm25_a = `PM2.5_CF_1_ug/m3_A`,
pm25_b = `PM2.5_CF_1_ug/m3_B`,
temp = Temperature_F_A,
rh = `Humidity_%_A`)
# create hourly pm column
AQ_df$hour = as.POSIXlt(AQ_df$datetime)$hour
# Convert numeric values to a numeric class
AQ_df$pm25_a = as.numeric(AQ_df$pm25_a)
AQ_df$pm25_b = as.numeric(AQ_df$pm25_b)
AQ_df$temp = as.numeric(AQ_df$temp)
AQ_df$rh = as.numeric(AQ_df$rh)
# clean dataframe:
## complete missing parts of the time series
(AQ_df = AQ_df %>%
dplyr::select(ID, Name, Lon, Lat, Location, datetime, pm25_a, pm25_b, temp, rh) %>%
group_by(ID) %>%
# add in NAs to timeseries to calculate % complete (before missing time periods were just absent and not NAs in the dataset)
complete(datetime = seq(min(datetime), max(datetime), by = "10 min")))
## add time period classification: "pre-fire", "during fire", "post fire"
(time_period_classification = AQ_df %>%
mutate(time_period = ifelse(datetime<as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")), "pre_fire_period",
ifelse(datetime>=as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")) & datetime<= as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "fire_period",
ifelse(datetime>as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "post_fire_period", "other time")))))
## factor to order time periods
time_period_classification$time_period = factor(time_period_classification$time_period, levels = c("pre_fire_period","fire_period","post_fire_period","other time"))
## add in data completeness: "complete data", "sensor offline during fire did/did not return online", "sensor adding during fire did/did not return online", "sensor added after fire"
(time_period_classification = time_period_classification %>%
group_by(ID,time_period) %>%
dplyr::summarize(
complete_a= sum(complete.cases(pm25_a))/n()*100
#complete_b = sum(complete.cases(pm25_b))/n()*100
) %>%
pivot_wider(names_from = time_period, values_from = complete_a) %>%
rowwise() %>%
mutate(Status = case_when(
fire_period >= 75 & post_fire_period >= 85 & pre_fire_period >= 95 ~ "Complete data throughout fire period",
fire_period < 75 & post_fire_period >= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, returned online",
fire_period < 75 & post_fire_period <= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, did not return online",
fire_period < 75 & post_fire_period <= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, did not return online",
fire_period < 75 & post_fire_period >= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, returned online",
is.na(fire_period) & is.na(pre_fire_period) ~ "Sensor came online after fire"
)))
## factorize the status so it shows up better in maps
time_period_classification$Status = factor(time_period_classification$Status, levels = c("Complete data throughout fire period", "Sensor offline during fire, returned online", "Sensor offline during fire, did not return online", "Sensor added during fire, returned online", "Sensor added during fire, did not return online", "Sensor came online after fire"))
time_period_classification
# create a key coordinating the sensor ID number to its lat/lon
(id_key = AQ_df %>%
group_by(ID) %>%
dplyr::select(ID, Lon, Lat, Name, Location) %>%
unique() %>%
na.omit()
)
get_sensor_info <- function(ID) {
# for each id, get the rest of the information (including lat/loing)
info = id_key[id_key$ID == ID,]
# create a spatial version of info
info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
# intersect ID info with the county map to determine location
city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
city = city_intersect$ZONEDESC
info = info %>%
mutate(city = ifelse(length(city) == 0, NA, city)) %>%
dplyr::select(-Name)
return(info)
}
id_key %>% mutate(city = get_sensor_info(ID))
id_key %>% mutate(city = get_sensor_info(ID)$Location)
id_key %>% mutate(city = get_sensor_info(ID)$Name)
id_key %>% mutate(city = get_sensor_info(ID))
id_key %>% mutate(city = get_sensor_info(ID))
t = id_key %>% mutate(city = get_sensor_info(ID))
t$city
info = get_sensor_info(id_key$ID)
info
# use helper function to get municipality for each sensor
sensor_info = get_sensor_info(id_key$ID)
# combine fire period classification and ID data into sensor stats
rbind(sensor_info,time_period_classification)
# combine fire period classification and ID data into sensor stats
rbind(sensor_info,time_period_classification,by="ID")
# combine fire period classification and ID data into sensor stats
cbind(sensor_info,time_period_classification,by="ID")
# combine fire period classification and ID data into sensor stats
cbind(sensor_info,time_period_classification)
# combine fire period classification and ID data into sensor stats
cbind(sensor_info,time_period_classification, by = "ID")
# combine fire period classification and ID data into sensor stats
sensor_stats = cbind(sensor_info, time_period_classification, by = "ID")
sensor_stats
sensor_stats %>%
dplyr::select(-`ID...6`)
sensor_stats %>%
dplyr::select(-`ID...6`,-by) %>%
rename(ID = `ID...1`)
# combine fire period classification and ID data into sensor stats
sensor_stats = cbind(sensor_info, time_period_classification, by = "ID")
sensor_stats %>%
dplyr::select(-`ID...6`,-by) %>%
rename(ID = `ID...1`)
sensor_stats %>%
dplyr::select(-`ID...6`,-by)
sensor_stats %>%
dplyr::select(-`ID...6`,-by) %>%
dplyr::rename(ID = `ID...1`)
# AQ df for cleaning
write.csv(AQ_df,"/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data")
# AQ df for cleaning
write.csv(AQ_df,"/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
knitr::opts_chunk$set(echo = TRUE)
dir
# directory where sensor data is stored
dir = "/Users/esrieves/Documents/GitHub/marshall_fires"
dir
AQ_df = read_csv("/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_df")
AQ_df = read_csv("/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_df.csv")
AQ_df = read_csv("/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
AQ_df
AQ_df = AQ_df[,-1]
AQ_df
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(readr)
library(plyr)
# directory where sensor data is stored
dir = "/Users/esrieves/Documents/GitHub/marshall_fires"
# read in the file
AQ_df = read_csv("/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
# drop the index column
AQ_df = AQ_df[,-1]
# determine if data points are valid based on EPA logic
AQ_df = AQ_df %>%
rowwise() %>%
mutate(
a_b_agree =
# if eiher pm25_a or pm25_b is NA, disagree
ifelse(is.na(pm25_a) | is.na(pm25_b), "disagree",
# if pm25_a is less than pm25_b + 5
ifelse(pm25_a <= pm25_b + 5,"agree",
# if pm25_a is more than pm25_b - 5
ifelse(pm25_a >= pm25_b - 5, "agree",
# if pm25_a is within 70% relative difference
ifelse(
abs(((pm25_a - pm25_b)/pm25_b)*100) >= 70 | abs(((pm25_b - pm25_a)/pm25_a)*100) >= 70, "agree","disagree")))))
# select only valid points, average a and b channels for those data
AQ_df_valid = AQ_df %>%
dplyr::filter(a_b_agree == "agree") %>%
rowwise() %>%
mutate(pm_avg = mean(c(pm25_a,pm25_b)))
# summary stats
AQ_df %>% group_by(ID) %>% summarise(count_A_na = sum(is.na(pm25_a)),
count_B_na = sum(is.na(pm25_b)),
pct_A_na = (count_A_na/length(pm25_a))*100,
pct_B_na = (count_B_na/length(pm25_b))*100)
# overall percent valid
AQ_df %>% group_by(ID) %>% summarise(pct_valid = sum(a_b_agree == "agree")/length(pm25_a) * 100)
library(gtsummary)
## work on the cross table -- maybe add in time period and municipality as organizing vars
AQ_df %>% tbl_cross(row = ID, col = a_b_agree, percent = "row") %>% as_gt()
# summary stats
AQ_df %>% group_by(ID) %>% summarise(count_A_na = sum(is.na(pm25_a)),
count_B_na = sum(is.na(pm25_b)),
pct_A_na = (count_A_na/length(pm25_a))*100,
pct_B_na = (count_B_na/length(pm25_b))*100)
# overall percent valid
AQ_df %>% group_by(ID) %>% summarise(pct_valid = sum(a_b_agree == "agree")/length(pm25_a) * 100)
## work on the cross table -- maybe add in time period and municipality as organizing vars
AQ_df %>% tbl_cross(row = ID, col = a_b_agree, percent = "row") %>% as_gt()
# linear piecewise correction from the EPA
## question about using the pm avg or just the a channel
## confirm that this is the best correction
(corrected_AQ = AQ_df_valid %>% rowwise() %>% mutate(
corrected_pm = ifelse(pm_avg < 50, (0.52 * pm_avg - 0.088 * rh + 5.75),
ifelse(pm_avg >= 50 | pm_avg < 299, (0.786 * pm_avg - 0.086 * rh + 5.75),
ifelse(pm_avg >= 229, (0.69 * pm_avg + 8.84 * 0.001 * pm_avg^2 + 2.97), NA)))))
# select only valid points, average a and b channels for those data
AQ_df_valid = AQ_df %>%
dplyr::filter(a_b_agree == "agree")
AQ_df_valid %>%
rowwise() %>%
mutate(pm_avg = mean(c(pm25_a,pm25_b)))
# select only valid points, average a and b channels for those data
(AQ_df_valid = AQ_df %>%
dplyr::filter(a_b_agree == "agree") %>%
rowwise() %>%
mutate(pm_avg = mean(c(pm25_a,pm25_b))))
# linear piecewise correction from the EPA
## question about using the pm avg or just the a channel
## confirm that this is the best correction
AQ_df_valid
(corrected_AQ = AQ_df_valid %>%
rowwise() %>%
mutate(
corrected_pm = ifelse(pm_avg < 50, (0.52 * pm_avg - 0.088 * rh + 5.75),
ifelse(pm_avg >= 50 | pm_avg < 299, (0.786 * pm_avg - 0.086 * rh + 5.75),
ifelse(pm_avg >= 229, (0.69 * pm_avg + 8.84 * 0.001 * pm_avg^2 + 2.97), NA)))))
AQ_df %>% tbl_cross(row = Location, col = a_b_agree, percent = "row") %>% as_gt()
AQ_df %>% tbl_cross(row = Name, col = a_b_agree, percent = "row") %>% as_gt()
# determine if data points are valid based on EPA logic
AQ_df
## work on the cross table -- maybe add in time period and municipality as organizing vars
AQ_df %>% tbl_cross(row = ID, col = a_b_agree, percent = "row") %>% as_gt()
AQ_df %>% tbl_cross(row = Location, col = a_b_agree, percent = "row") %>% as_gt()
AQ_df %>% tbl_cross(row = Name, col = a_b_agree, percent = "row") %>% as_gt()
# rename name to city
AQ_df %>% dplyr::rename(city = Name)
# rename name to city
AQ_df AQ_df %>% dplyr::rename(city = Name)
# rename name to city
AQ_df = AQ_df %>% dplyr::rename(city = Name)
AQ_df %>% tbl_cross(row = Name, col = a_b_agree, percent = "row") %>% as_gt()
AQ_df %>% tbl_cross(row = city, col = a_b_agree, percent = "row") %>% as_gt()
# rename name to city
AQ_df = AQ_df %>% dplyr::rename(city = Name)
AQ_df
# linear piecewise correction from the EPA
## question about using the pm avg or just the a channel
## confirm that this is the best correction
(corrected_AQ = AQ_df_valid %>%
rowwise() %>%
mutate(
corrected_pm = ifelse(pm_avg < 50, (0.52 * pm_avg - 0.088 * rh + 5.75),
ifelse(pm_avg >= 50 | pm_avg < 299, (0.786 * pm_avg - 0.086 * rh + 5.75),
ifelse(pm_avg >= 229, (0.69 * pm_avg + 8.84 * 0.001 * pm_avg^2 + 2.97), NA)))))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(readr)
library(plyr)
library(gtsummary)
# directory where sensor data is stored
dir = "/Users/esrieves/Documents/GitHub/marshall_fires"
# read in the file
AQ_df = read_csv("/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
# drop the index columnx
AQ_df = AQ_df[,-1]
# rename name to city
AQ_df = AQ_df %>% dplyr::rename(city = Name)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(readr)
library(plyr)
library(gtsummary)
# directory where sensor data is stored
dir = "/Users/esrieves/Documents/GitHub/marshall_fires"
# read in the file
AQ_df = read_csv("/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
# drop the index columns
AQ_df = AQ_df[,-1]
# rename name to city
AQ_df = AQ_df %>% dplyr::rename(city = Name)
# determine if data points are valid based on EPA logic
AQ_df = AQ_df %>%
rowwise() %>%
mutate(
a_b_agree =
# if eiher pm25_a or pm25_b is NA, disagree
ifelse(is.na(pm25_a) | is.na(pm25_b), "disagree",
# if pm25_a is less than pm25_b + 5
ifelse(pm25_a <= pm25_b + 5,"agree",
# if pm25_a is more than pm25_b - 5
ifelse(pm25_a >= pm25_b - 5, "agree",
# if pm25_a is within 70% relative difference
ifelse(
abs(((pm25_a - pm25_b)/pm25_b)*100) >= 70 | abs(((pm25_b - pm25_a)/pm25_a)*100) >= 70, "agree","disagree")))))
# select only valid points, average a and b channels for those data
(AQ_df_valid = AQ_df %>%
dplyr::filter(a_b_agree == "agree") %>%
rowwise() %>%
mutate(pm_avg = mean(c(pm25_a,pm25_b))))
# summary stats
AQ_df %>% group_by(ID) %>% summarise(count_A_na = sum(is.na(pm25_a)),
count_B_na = sum(is.na(pm25_b)),
pct_A_na = (count_A_na/length(pm25_a))*100,
pct_B_na = (count_B_na/length(pm25_b))*100)
# overall percent valid
AQ_df %>% group_by(ID) %>% summarise(pct_valid = sum(a_b_agree == "agree")/length(pm25_a) * 100)
## work on the cross table -- maybe add in time period and municipality as organizing vars
AQ_df %>% tbl_cross(row = ID, col = a_b_agree, percent = "row") %>% as_gt()
# linear piecewise correction from the EPA
## question about using the pm avg or just the a channel
## confirm that this is the best correction
(corrected_AQ = AQ_df_valid %>%
rowwise() %>%
mutate(
corrected_pm = ifelse(pm_avg < 50, (0.52 * pm_avg - 0.088 * rh + 5.75),
ifelse(pm_avg >= 50 | pm_avg < 299, (0.786 * pm_avg - 0.086 * rh + 5.75),
ifelse(pm_avg >= 229, (0.69 * pm_avg + 8.84 * 0.001 * pm_avg^2 + 2.97), NA)))))
