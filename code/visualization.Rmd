---
title: "marshall_visualizations"
author: "Emma Rieves & Zac Clement"
date: "Created 4/27/2022, Updated 7/5/2022, Updated ESR 12/1/22"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load the Libraries
```{r}
library(sf)
library(dplyr)
library(sp)
library(gstat)
library(raster)
library(tidyverse)
library(spacetime)
library(stringr)
library(readr)
library(leaflet)
library(gridExtra)
library(gifski)
library(nngeo)

# to make the spherical geometry errors go away
sf::sf_use_s2(FALSE)
```

# Import Data
## Air quality & sensor data
```{r}
# AIR QUALITY
## corrected air quality
AQ_df = read.csv("../intermediary_outputs/corrected_AQ_data.csv")

## sensor info
sensors = read.csv("../intermediary_outputs/sensor_data_full.csv")
```

## Geometries
```{r}
# BOUNDARIES
# Boulder boundaries
BO_CO = st_read("../GIS_inputs_destruction_fireboundary/Boulder_county_munis/Municipalities.shp")
fire_counties = BO_CO %>%
  dplyr::filter(ZONEDESC == "Louisville" |
                  ZONEDESC == "Superior" |
                  ZONEDESC == "Broomfield" |
                  ZONEDESC == "Lafayette" |
                  ZONEDESC == "Boulder")
# Boulder Boundary
boulder_precincts = st_read("../GIS_inputs_destruction_fireboundary/Unincorporated_Boulder/Unincorporated_Boulder.shp")
# Broomfield Boundary
broomfield_precincts = st_read("../GIS_inputs_destruction_fireboundary/Broomfield_Precincts/Precincts.shp")
# Westminster Boundary
westminster_city = st_read("../GIS_inputs_destruction_fireboundary/Westminster_CityLimits/CityLimits.shp")

# prg - set projection for everything
prg = "+proj=utm +zone=13 +datum=WGS84 +units=km +no_defs"

# make all the precincts have the same columns (we don't need a lot of the data)
boulder_precincts <- boulder_precincts %>%
  dplyr::select(c(OBJECTID, geometry, SHAPEarea, SHAPElen))
broomfield_precincts <- broomfield_precincts %>%
  mutate(SHAPEarea = Shape__Are,
         SHAPElen = Shape__Len,
         OBJECTID = OBJECTID + 1000) %>% # add 1,000 to the broomfield precincts to get rid of overlapping labels
  dplyr::select(-c("GlobalID", "GIS_ID", "PRECINCT_N", "MAP_COLOR", "USER_COMME", "LAST_UPDAT", "Shape__Are", "Shape__Len"))
westminster_city <- westminster_city %>%
  mutate(SHAPEarea = ShapeSTAre,
         SHAPElen = ShapeSTLen) %>%
  dplyr::select(c(OBJECTID, geometry, SHAPEarea, SHAPElen))

# save proj string for fire-affected counties to use for other transformations
#prg = raster::crs(fire_counties,asText=TRUE)

# all boundaries joined together
all_bounds <- st_transform(boulder_precincts, crs = prg) %>%
  rbind(st_transform(broomfield_precincts, crs=prg)) %>%
  rbind(st_transform(westminster_city, crs=prg))

# remove the holes that existed so we just have the outside boundary of this area
all_bounds <- all_bounds %>%
  st_union() %>%
  st_remove_holes()

# Marshall fire boundary
wfigs_fire = st_read("../GIS_inputs_destruction_fireboundary/WFIGS_-_Wildland_Fire_Perimeters_Full_History/FH_Perimeter.shp")

# filter fire to marshall fire only; update crs
marshall_fire = wfigs_fire %>% filter(poly_Incid == "Marshall") %>% st_transform(crs = st_crs(prg))
```

After getting the boundary data, we need to read in the geographic locations of buildings that were damaged or destroyed by the fire.
```{r}
# BUILDINGS
# Read in destroyed home and building sites
destroyed_homes = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_homes.shp")
damaged_homes = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/damaged_homes.shp")
destroyed_businesses = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_businesses.shp")
damaged_businesses = st_read("../GIS_inputs_destruction_fireboundary/output_damage_files/damaged_businesses.shp")

# create 'type' column; select only the columns needed; make projection string match
destroyed_homes = destroyed_homes %>%
  mutate(type = "destroyed - residential") %>%
  dplyr::rename(jurisdiction = JURISDICTI) %>%
  dplyr::select(jurisdiction, type, latlong, geometry) %>%
  st_transform(crs = st_crs(prg))
destroyed_businesses = destroyed_businesses %>%
  mutate(type = "destroyed - non-residential") %>%
  dplyr::rename(jurisdiction = Jurisdicti) %>%
  dplyr::select(jurisdiction, type, latlong, geometry) %>%
  st_transform(crs = st_crs(prg))
damaged_homes = damaged_homes %>%
  mutate(type = "damaged - residential") %>%
  dplyr::rename(jurisdiction = JURISDICTI) %>%
  dplyr::select(jurisdiction, type, latlong, geometry) %>%
  st_transform(crs = st_crs(prg))
damaged_business = damaged_businesses %>%
  mutate(type = "damaged - non-residential") %>%
  dplyr::rename(jurisdiction = Jurisdicti) %>%
  dplyr::select(jurisdiction, type, latlong, geometry) %>%
  st_transform(crs = st_crs(prg))

# combine into one df, using the "," indicator keeps decimal points
destroyed_damaged = rbind(destroyed_homes,
                          destroyed_businesses,
                          damaged_homes,
                          damaged_business) %>%
  separate(latlong, c("lat","long"), ",", convert = FALSE)

# change lat and long to floats
destroyed_damaged$lat = as.double(destroyed_damaged$lat)
destroyed_damaged$long = as.double(destroyed_damaged$long)
```

# Leaflet Maps

```{r}
# create "location status" column for mapping]
sensors$loc_status = str_c(sensors$Location, " - ", sensors$Status)
# make sure we only have valid sensors that have data
sensors = na.omit(sensors)
```

```{r}
# set factors to false
options(stringsAsFactors = FALSE)

# create palette for destroyed and damaged building colors for leaflet map
pal = colorFactor(c("red","orange","magenta","light pink"), domain = unique(destroyed_damaged$type))

## create indoor/outdoor icons
sensorIcon = awesomeIconList(
  "inside" = makeAwesomeIcon(
    icon = "home",
    iconColor = "white",
    markerColor = "blue",
    library = "fa"
  ),
  "outside" = makeAwesomeIcon(
    icon = "tree",
    iconColor = "white",
    markerColor = "green",
    library = "fa"
  ))

sensors

# interactive map of damaged/destroyed buildings and AQ sensors (indoor/outdoor)
# chose not to do icons for destroyed/damaged buildings because the map got really busy
fire_destruction_and_AQ_plot = leaflet(sensors) %>%
    addTiles() %>% 
    addAwesomeMarkers(icon = ~sensorIcon[Location],
                      lng = sensors$Lon, lat = sensors$Lat) %>% 
    addCircleMarkers(color = ~pal(destroyed_damaged$type),
                     radius = 3.5,
                     opacity = 1,
                     lng = destroyed_damaged$long, lat = destroyed_damaged$lat) %>%
    addLegend("topright", pal = pal, values = ~destroyed_damaged$type,
              title = "Fire damage")
```

Leaflet map that colors each sensor icon based on the time period it has data:
```{r}
# create palette for location + time period
timeSensorIcon = awesomeIconList(
  "inside - Complete data throughout fire period" = makeAwesomeIcon(
    icon = "home",
    iconColor = "white",
    markerColor = "green",
    library = "fa"
  ),
  "inside - Sensor offline during fire, did not return online" = makeAwesomeIcon(
    icon = "tree",
    iconColor = "white",
    markerColor = "gray",
    library = "fa"
  ),
  "inside - Sensor offline during fire, returned online" = makeAwesomeIcon(
    icon = "home",
    iconColor = "white",
    markerColor = "blue",
    library = "fa"
  ),
   "outside - Complete data throughout fire period" = makeAwesomeIcon(
    icon = "home",
    iconColor = "white",
    markerColor = "green",
    library = "fa"
  ),
  "outside - Sensor offline during fire, did not return online" = makeAwesomeIcon(
    icon = "tree",
    iconColor = "white",
    markerColor = "gray",
    library = "fa"
  ),
  "outside - Sensor offline during fire, returned online" = makeAwesomeIcon(
    icon = "tree",
    iconColor = "white",
    markerColor = "blue",
    library = "fa"
  ))

# map based on when sensor was added
(fire_destruction_and_AQ_plot = leaflet(sensors) %>%
    addTiles() %>% 
    addAwesomeMarkers(icon = ~timeSensorIcon[loc_status],
                      lng = sensors$Lon, lat = sensors$Lat) %>% 
    addCircleMarkers(color = ~pal(destroyed_damaged$type),
                     radius = 3.5,
                     opacity = 1,
                     lng = destroyed_damaged$long, lat = destroyed_damaged$lat) %>%
    addLegend("topright", pal = pal, values = ~destroyed_damaged$type,
              title = "Fire damage") %>%
    addLegend("bottomright",
              pal = colorFactor(c("green","gray","blue"), domain = unique(sensors$Status)),
              values = ~sensors$Status, title = "Sensor Time Info"))
```

# Making grid for interpolation
We create a 50x50 grid for interpolation. This results in the resolution being 0.009372036 km wide by 0.007888942 km tall.
```{r}
grd <- SpatialPixels(SpatialPoints(as_Spatial(st_make_grid(all_bounds, n=50))), proj4string = prg)
plot(grd)

grd <- grd[as_Spatial(all_bounds),]
plot(grd)

grd <- spTransform(grd, CRS(prg))


r_grd <- raster(grd, nrows=50, ncols=49) # creating a 50x50 object gives some weird visuals
```

# Create spatial objects
To do more in-depth spatial analysis and mapping, we need to convert our data into spacetime objects.
```{r}
library(lubridate)
# Cleaning to prepare to make spatial objects
# first, re-format datetime as a xts object
AQ_df$datetime = as.POSIXct(AQ_df$datetime)

# filter out extreme values from Eisenhower dr PA sensor
AQ_df%>% filter(ID == 91877 & corrected_pm > 1000) # 319 over 1000
AQ_df[AQ_df$ID == 91877 & AQ_df$corrected_pm > 1000, ] = NA
AQ_df %>% filter(ID == 91877 & corrected_pm > 1000) # 0 obs over 1000

# use floor date (lubridate) to effectively resample into daily posixct object, then calculate daily average for each ID
daily_AQ_df = AQ_df %>% mutate(day = floor_date(datetime, "day")) %>% group_by(ID,day) %>% summarise(daily_mean = mean(corrected_pm)) # 1622 distinct rows

# create dataframe of sensor data to merge with daily data
sensor_data_df = AQ_df %>% dplyr::select(ID, Name, Lon, Lat, Location)
sensor_data_df = sensor_data_df %>% na.omit() %>% distinct() # 53 distinct sensors (one row of NAs)

# merge daily AQ df and sensor data df, distinct removes any duplicates that would arise
AQ_data = merge(sensor_data_df,daily_AQ_df,by="ID") # should be 1621 rows
AQ_data 

# check for NAs -- not all sensors have all data, 42 sensors that don't have data (but more that are just excluded in the index)
sapply(AQ_data, function(x) sum(is.na(x)))

```

```{r}
# cleaning -- remove wonky sensors
AQ_data = AQ_data %>%
  # already filtered out Eisenhower Dr outliers (PM > 1000) above before aggregation
  # exclude any non-outside sensors
  filter(Location == "outside") %>%
  # drop sensor 112606 (Wildwood Ct) because it has 0% confidence according to PurpleAir map (per Zac)
  filter(ID != 112606)

# determine the number of sensors for days < Feb 4 -- looks like there are 48 overall with 30 sensors during various phases of the time period (1/8 - 2/4)
AQ_data %>% group_by(ID) %>% summarise(count = n())
AQ_data %>% group_by(ID) %>% filter(day < "2022-02-05") %>% summarise(count = n())

```


```{r}
# THIS SECTION DETERMINES: which sensors have enough data within the study period (1/8 - 2/5) to be used to create the study period average used in IDW.

# determine which IDs have enough data within the study period to be used to create the monthly average variable
study_completeness = AQ_data %>% 
  # filter to include only study period
  filter(day >= "2022-01-08" & day < "2022-02-05") %>% 
  group_by(ID) %>%
  # create completeness variable, need 70% of days in period to be complete
  # 28 days in study period, use this to determine complete pct
  summarise(day_count = n(),
            complete_pct = (day_count/28) * 100,
            is_complete = ifelse(complete_pct >= 70, "complete", "incomplete")) # 8 sensors are complete

# merge with completeness variable with AQ_data to create AQ_data_study_period
(AQ_data_study_period = merge(study_completeness %>% dplyr::select(ID,is_complete),AQ_data, by = "ID")) # only 1174 rows, probably because of removed dates

# these seem really low...
AQ_data_study_period = AQ_data_study_period %>% 
  # filter to include only the study period
  filter(day >= "2022-01-08" & day < "2022-02-05") %>% 
  # filter to only include complete sensors
  filter(is_complete == "complete") %>%
  # groupby ID to calculate the "study mean" -- the mean over the study time period
  group_by(ID) %>% 
  summarise(study_mean = mean(daily_mean),
            Lat = Lat,
            Lon = Lon) %>%
  # keep distinct observations
  distinct()


AQ_data_study_period
# to gut check
AQ_data %>% filter(ID == 65669)
```


```{r}
# create spatial points object for each sensor
PA_sensors = SpatialPoints(AQ_data[!duplicated(AQ_data$ID), c("Lon", "Lat")],proj4string = CRS(prg))
summary(PA_sensors) # 48 sensors

# construct spatiotemporal object
PA_STFDF = stConstruct(AQ_data, space = c("Lon","Lat"), time = "day", crs = CRS(prg), SpatialObj = PA_sensors)

# turn ST object into a STFDF (stationary points)
PA_STFDF = as(PA_STFDF,"STFDF")

# see summary of data
summary(PA_STFDF)
# object class
class(PA_STFDF)
# object dimensions
dim(PA_STFDF)
```

# Descriptive plots
```{r}
# not all sensors have complete data
study_period = AQ_data %>% 
  # filter to correct dates
  filter(day >= "2022-01-08" & day < "2022-02-05") %>%
  # no data for this sensor
  filter(ID != 119691)

# max value during the time period
max(study_period$daily_mean, na.rm = TRUE)

# create plot -- png makes blurry I recommend screenshot
#png("../images/manuscript/time_series.png",width = 850, height = 600)
print(ggplot(data = study_period, aes(x = day, y = daily_mean)) +
  geom_line() +
  facet_wrap(~ID) +
  labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
       title = expression("Time series of daily average PM"[2.5]*" for each sensor from January 8 to February 4, 2022")))
#dev.off()
```

```{r}
#make id a char for grouping
study_period$ID = as.character(study_period$ID)
print(ggplot(data = study_period, aes(x = day, y = daily_mean, color=ID)) +
  geom_line() +
  labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
       title = expression("Time series of daily average PM"[2.5]*" for each sensor from January 8 to February 4, 2022")))
```

```{r}
# create figure asked for by Joost and Jon
study_period2 = AQ_data %>% 
  # filter to correct dates
  filter(day < "2022-02-05") %>%
  # no data for this sensor
  filter(ID != 119691)

#make id a char for grouping
study_period2$ID = as.character(study_period2$ID)
print(ggplot(data = study_period2, aes(x = day, y = daily_mean, color=ID)) +
  geom_line() +
  labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
       title = expression("Purple Air daily average PM"[2.5]*"")))
```



# IDW
## daily plots
```{r}
# determine AQ max and min values to create upper and lower bounds for mapping -- use PAstfdf so mapping is consistent across all maps
(aq_max = max(PA_STFDF[,"2022-01-01::2022-02-04"]@data$daily_mean, na.rm = TRUE)) # 172.2092
(aq_min = min(PA_STFDF[,"2022-01-01::2022-02-04"]@data$daily_mean, na.rm = TRUE)) # 0.12695

summary(PA_STFDF[,"2022-01-08::2022-02-04"])

# create equal interval breaks based on aq min and max
(breaks2 = seq(from=aq_min, to=aq_max, length.out = 6))

# helper function to plot
plot_idw_raster_daily <- function(idw, title) {
  r <- raster::rasterize(idw, r_grd, field="var1.pred")
  extent(r) <- st_bbox(all_bounds)
  r_df <- r %>%
    rasterToPoints() %>%
    data.frame() %>%
    mutate(cuts = cut(layer, breaks=breaks2, na.omit=T))
  
  return(ggplot() +
          geom_tile(data=r_df, aes(x=x, y=y, fill=cuts))+
          scale_fill_brewer(palette = "YlOrRd", name=expression("PM"[2.5]*" (µg/m"^3*")"), drop=FALSE) +
          geom_sf(data=marshall_fire$geometry, col="gray20", alpha=0.1) +
          geom_point(data = r_df[!is.na(r_df$daily_mean), ], aes(x = x, y = y)) + 
          labs(y = "Latitude", x = "Longitude",
               caption = "Border represents Marshall Fire perimeter") +
          ggtitle(title))
}

```

```{r}
# jan 1 - feb 4th inclusive
for (j in 1:2) {
  date <- paste("2022-0", j, sep="")
  if (j == 1) {
    range = 1:31
  } else {
    range = 1:4
  }
  
  for (i in range) {
    if (i < 10) {
      i <- paste("0", i, sep="")
    }
    day1 <- paste(date, "-", i, sep="")
    #print(day1)
    daily_data <- PA_STFDF[, day1]
    print(daily_data@data)
    #print(data@data)
    N = n_distinct(daily_data@data$ID)
    
    idw <- idw(daily_mean~1, daily_data[!is.na(daily_data$daily_mean),], grd, idp = 3)
    #idw <- idw(daily_mean~1, data, grd, idp = 3)
    #png(paste("../images/jan1-feb4_V2/", day1, ".png", sep=''))
    print(plot_idw_raster_daily(idw, paste(day1, "IDW, N = ", N)))
    #dev.off()
    }
}
```

## average plot
```{r}
(monthly_avg_data = AQ_data %>% 
  filter(day >= "2022-01-08" & day < "2022-02-05") %>% 
  group_by(ID) %>% 
  # use 28 in calculating the percent because 28 days in the period
  dplyr::summarise(N = n(),
                   # include Lat and Lon to reduce need for joins
                   Lat = Lat,
                   Lon = Lon,
                   pct_complete = (N/28) * 100,
                   is_usable = ifelse(pct_complete >= 70, "usable", "not usable"),
                   mean_pm = mean(daily_mean, na.rm=TRUE)))

(monthly_avg_data = monthly_avg_data %>% filter(is_usable == "usable") %>% distinct() %>% as.data.frame())
row.names(monthly_avg_data) = monthly_avg_data$ID
```

```{r}
# make spatial object of monthly average
# start by creating spatial stations
coords = cbind(monthly_avg_data$Lon, monthly_avg_data$Lat)
row.names(coords) = monthly_avg_data$ID

# create spatial data frame of points
(monthly_avg_data_spatial = SpatialPointsDataFrame(coords, monthly_avg_data, proj4string = CRS(prg), match.ID = TRUE))
monthly_avg_data_spatial@data
```

```{r}
monthly_idw = idw(mean_pm~1, monthly_avg_data_spatial, grd, idp = 2)

#png("../images/manuscript/period_average.png", width = 700, height = 350)
tiff("../images/manuscript/period_average.tiff", units = "in", width = 12, height = 3.5, res = 600)
(monthly_idw_fig = plot_idw_raster_daily(idw = monthly_idw, title = expression("Inverse distance weighted interpolated PM"[2.5]*" concentrations averaged from January 8 to February 4, 2022, N = 9")))
dev.off()

```

```{r}
## USE THIS TO PRINT MANUSCRIPT GRAPHIC
r1 = raster::rasterize(monthly_idw, r_grd, field="var1.pred")
extent(r1) = st_bbox(all_bounds)
r_df1 = r1 %>%
  rasterToPoints() %>%
  data.frame() %>%
  mutate(cuts = cut(layer, breaks2, na.omit=T))

monthly_plot = ggplot() +
  geom_tile(data=r_df1, aes(x=x, y=y, fill=cuts))+
  scale_fill_brewer(palette = "YlOrRd", name=expression("PM"[2.5]*" (µg/m"^3*")"), drop=FALSE) +
  geom_sf(data=marshall_fire$geometry, col="gray20", alpha=0.1) +
  geom_point(data = r_df1[!is.na(r_df1$daily_mean), ], aes(x = x, y = y)) + 
  #ggtitle("Inverse distance weighted interpolated PM"[2.5]*" concentrations averaged from January 8 to February 4, 2022, N = 9") +
  labs(title = expression("IDW interpolated PM"[2.5]*" concentration, averaged from 1/8/22 - 2/4/22, N = 9"), y = "Latitude", x = "Longitude", caption = "Border represents Marshall Fire perimeter")


tiff("../images/manuscript/period_average.tiff", units = "in", width = 10, height = 3.5, res = 600)
monthly_plot
dev.off()
```


```{r}
sensors_in_monthly_avg = study_period %>%
  filter(ID %in% c("65669","68375","81149","85999","86477","91877","92321","136514","137686"))

write.csv(sensors_in_monthly_avg,"../intermediary_outputs/sensors_in_monthly_data.csv")

print(ggplot(data = sensors_in_monthly_avg, aes(x = day, y = daily_mean)) +
  geom_line() +
  facet_wrap(~ID) +
  labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
       title = expression("Time series of daily average PM"[2.5]*" for sensors used to create average from January 8 to February 4, 2022")))
```

```{r}
#make ID a char for goruping in fig
sensors_in_monthly_avg$ID = as.character(sensors_in_monthly_avg$ID)
print(ggplot(data = sensors_in_monthly_avg, aes(x = day, y = daily_mean, color=ID)) +
  geom_line() +
  labs(y = expression("Daily average PM"[2.5]*" (µg/m"^3*")"), x = "Date",
       title = expression("Time series of daily average PM"[2.5]*" for sensors used to create average from January 8 to February 4, 2022")))
```

```{r}
# using the weird numbers to confirm that this works without corrupting original version.. go back and change everything to this if need be
allbounds3 = all_bounds
allbounds3 = st_transform(allbounds3, crs=4326)
marshall2 = marshall_fire
marshall2 = st_transform(marshall2, crs = 4326)

(basemap = ggplot() +
  geom_sf(data = allbounds3) +
  geom_sf(data = marshall2))

basemap +
  geom_point(
    data = monthly_avg_data,
    aes(x=Lon,y=Lat,color=mean_pm)
  )
```





# PAST WORK
# Map of all sensors we have data for (many not online during the fire)
```{r}
ggplot(all_bounds, aes(geometry=geometry)) +
  geom_sf() +
  geom_sf(data=marshall_fire$geometry, fill="red", alpha=0.6) +
  geom_sf(data=st_as_sf(PA_sensors))
```

# Time series plots
```{r}
## Time series plots
stplot(PA_STFDF[,"2021-12-30::2021-12-31","temp"],mode="ts")
stplot(PA_STFDF[,"2021-12-30::2021-12-31","rh"],mode="ts")
stplot(PA_STFDF[,"2021-12-30::2021-12-31","corrected_pm"],mode="ts")
```

# Spacetime Plots
These show time on the y-axis, each sensor on the x-axis, and the color corresponds to the variable (temperature, relative humidity, or PM2.5).
```{r warning=FALSE}
stplot(PA_STFDF[,"2021-12-30::2021-12-31","temp"],mode="xt")
stplot(PA_STFDF[,"2021-12-30::2021-12-31","rh"],mode="xt")
stplot(PA_STFDF[,"2021-12-30::2021-12-31","corrected_pm"],mode="xt")
```

# Sensors With Complete Data
```{r}
# make an STFDF with only the sensors that had complete data for the fire
complete_sensors = sensors %>% filter(Status == "Complete data throughout fire period")
complete_STFDF = PA_STFDF[PA_STFDF@data$ID %in% complete_sensors$ID]
stplot(complete_STFDF[,,"corrected_pm"], mode="ts")
```
## Plotting the PM2.5 values for each sensor on 12/30/2021
```{r}
stplot(complete_STFDF[,"2021-12-30::2021-12-31","corrected_pm"], mode="ts")
stplot(complete_STFDF[,"2021-12-30::2021-12-31","corrected_pm"], mode="tp")
```

# Kriging
## Funky Data from Variograms
After running through this kriging methodology without removing any data points, there were a few sensors that had very strange values when looking through the variograms. This prompted me to plot them individually and examine the actual PM2.5 values for the fire period, to see if any of them seemed unreasonable. Both the Eisenhower Dr and 1333 Wildwood Ct sensors have values that are far too high, so those are dropped for our analysis done below.
```{r}
# Index 15 -> ID 60517, STANDLEY LAKE
#plot(fortify(PA_STFDF[PA_STFDF@data$ID == 60517]["2021-12-30::2022-01-02", "corrected_pm"]))
# looks normal

# Index 23 -> ID 91877, Eisenhower Dr
#plot(fortify(PA_STFDF[PA_STFDF@data$ID == 91877]["2021-12-30::2022-01-02", "corrected_pm"]))
# has incredibly large values midday Friday - Sunday, could be snow related?

# Index 36 -> ID 112606, 1333 Wildwood Ct., Boulder, CO 80305 sensor
#plot(fortify(PA_STFDF[PA_STFDF@data$ID == 112606]["2021-12-30::2022-01-02", "corrected_pm"]))
# looking at purpleair map, A channel is 'downgraded', 0% confidence

# Index 40 -> ID 119547, Broadlands sensor
#(fortify(PA_STFDF[PA_STFDF@data$ID == 119547]["2021-12-30::2022-01-02", "corrected_pm"]))
# reasonable values
```


Looking at Dec 30 - Jan 2, only outside sensors.
```{r warning=FALSE}
# get all of the data for the 12/30 to 1/2 timeframe
krig_data <- complete_STFDF[,"2021-12-30::2022-01-02"]
# limit to only outdoor sensors
krig_data <- krig_data[krig_data@data$Location == "outside"]
# drop sensor 112606 (Wildwood Ct) because it has 0% confidence according to PurpleAir map
krig_data <- krig_data[krig_data@data$ID != 112606]
# drop values for "Eisenhower Dr" sensor that are over 1000
krig_data@data[(krig_data@data$ID == 91877) &
                 (as.numeric(krig_data@data$corrected_pm) > 1000) &
                 !is.na(krig_data@data$corrected_pm),]$corrected_pm <- NA

# get rid of the weird sensor that might be throwing residuals off -- COMMENTED OUT FOR KRIG-DATA TO WORK
#krig_data <- krig_data[krig_data@data$Name != "ntsky"]

# reproject the data to utm
reproj <- "+proj=utm +zone=13 +datum=WGS84 +units=km"
#krig_data@sp <- spTransform(krig_data@sp, reproj)
krig_data@sp <- spTransform(krig_data@sp, reproj)

# aggregate from 10 min intervals to 1 hour intervals
krig_data <- aggregate(krig_data, by="hour", mean)

# split the data into the days we want for the fire period
dec30 <- krig_data[, "2021-12-30"]
dec31 <- krig_data[, "2021-12-31"]
jan1 <- krig_data[, "2022-01-01"]
jan2 <- krig_data[, "2022-01-02"]

# split each day into the time periods of focus
# pre-fire period: betweeen 00:00 - 11:00 MST on 12/30/2021
pre_fire <- aggregate(dec30, by="11 hours", mean)[, "2021-12-30 00:00:00"]
# fire period (11am - 5pm)
during_fire <- aggregate(dec30[, "2021-12-30 11:00:00::2021-12-30 16:00:00"], by="6 hours", mean)
# post-fire on 12/30/2021
evening_of_fire <- aggregate(dec30[, "2021-12-30 16:00:00::2021-12-31 00:00:00"], by="8 hours", mean)
# dec 31st
december31 <- aggregate(dec31, by="day", mean)
# january 1st
january1 <- aggregate(jan1, by="day", mean)
# january 2nd
january2 <- aggregate(jan2, by="day", mean)

periods <- c(pre_fire, during_fire, evening_of_fire, december31, january1, january2)

# create variograms for these time periods
pre_fire.vgm <- variogram(corrected_pm~1, pre_fire[!is.na(pre_fire$corrected_pm),])
during_fire.vgm <- variogram(corrected_pm~1, during_fire[!is.na(during_fire$corrected_pm),])
evening_of_fire.vgm <- variogram(corrected_pm~1, evening_of_fire[!is.na(evening_of_fire$corrected_pm),])
december31.vgm <- variogram(corrected_pm~1, december31[!is.na(december31$corrected_pm),])
january1.vgm <- variogram(corrected_pm~1, january1[!is.na(january1$corrected_pm),])
january2.vgm <- variogram(corrected_pm~1, january2[!is.na(january2$corrected_pm),])

# plot the variograms
plot(pre_fire.vgm)
# plot(during_fire.vgm) -- ERROR: need finite 'xlim' values
plot(evening_of_fire.vgm)
# plot(december31.vgm)  -- ERROR: need finite 'xlim' values
# plot(january1.vgm) -- ERROR: need finite 'xlim' values
# plot(january2.vgm) -- ERROR: need finite 'xlim' values
```


### Variogram Clouds
```{r}
# ERROR -- "error in evaluating the argument 'x' in selecting a method for function 'print': need finite 'xlim' values"
# for (period in periods) {
#   cloud <- variogram(corrected_pm~1, period[!is.na(period$corrected_pm), ], cloud=TRUE)
#   map <- variogram(corrected_pm~1, period[!is.na(period$corrected_pm), ], map=TRUE, cutoff=20, width=2)
#   print(plot(cloud))
#   print(plot(map))
# }

```


No good variogram models!!!!

We're going to try autofitting variograms & models to the variograms to see if this returns anything different than above.
```{r}

pre_fire.fitted <- fit.variogram(pre_fire.vgm, vgm(c("Exp", "Sph", "Gau", "Mat")))
pre_fire.fitted

plot(pre_fire.vgm, model=pre_fire.fitted)

#during_fire.fitted <- fit.variogram(during_fire.vgm, vgm(c("Exp", "Sph", "Gau", "Mat"))) -- ERROR: "object should be of class gstatVariogram or variogramCloud"
#during_fire.fitted

#plot(during_fire.vgm, model=during_fire.fitted)

evening_of_fire.fitted <- fit.variogram(evening_of_fire.vgm, vgm(c("Exp", "Sph", "Gau", "Mat")))
evening_of_fire.fitted

plot(evening_of_fire.vgm, model=evening_of_fire.fitted)

# december31.fitted <- fit.variogram(december31.vgm, vgm(c("Exp", "Sph", "Gau", "Mat"))) -- ERROR: "object should be of class gstatVariogram or variogramCloud"
# december31.fitted
# 
# plot(december31.vgm, model=december31.fitted)

# january1.fitted <- fit.variogram(january1.vgm, vgm(c("Exp", "Sph", "Gau", "Mat"))) -- ERROR: "object should be of class gstatVariogram or variogramCloud"
# january1.fitted
# 
# plot(january1.vgm, model=january1.fitted)

# january2.fitted <- fit.variogram(january2.vgm, vgm(c("Exp", "Sph", "Gau", "Mat"))) -- ERROR: "object should be of class gstatVariogram or variogramCloud"
# january2.fitted
# 
# plot(january1.vgm, model=january2.fitted)
```

These models are no good (some can't even find models), so kriging isn't going to be working with this data.

### Breaking down into smaller timeframes
Let's see if things look different when we examine each hour, instead of aggregating into ~6 hour periods.
```{r}
for (i in 0:11) {
  if (i < 10) {
    i <- paste("0", i, sep="")
  }
  t <- paste("2021-12-30 ", i, ":00:00", sep="")
  i.data <- dec30[, t]
  i.vgm <- variogram(corrected_pm~1, i.data[!is.na(i.data$corrected_pm),])
  # print(plot(i.vgm, main=paste("Variogram at", t)))
  i.fitted <- fit.variogram(i.vgm, vgm(c("Exp", "Sph", "Gau", "Mat")))
  print(i.fitted)
  print(plot(i.vgm, model=i.fitted, main=paste("Variogram at", t)))
}
```

Still no good variograms!

## Making grid for interpolation
We create a 50x50 grid for interpolation. This results in the resolution being 0.009372036 km wide by 0.007888942 km tall.
```{r}
grd <- SpatialPixels(SpatialPoints(as_Spatial(st_make_grid(all_bounds, n=50))), proj4string = prg)
plot(grd)

grd <- grd[as_Spatial(all_bounds),]
plot(grd)

grd <- spTransform(grd, CRS(reproj))
```

## Plotting Corrected PM

This is just used to check the data from the sensors, and visualize if any of the sensors have odd values.
```{r}
plot(pre_fire$corrected_pm)
plot(during_fire$corrected_pm)
plot(evening_of_fire$corrected_pm)
plot(december31$corrected_pm)
plot(january1$corrected_pm)
plot(january2$corrected_pm)
```


## IDW
Inverse distance weighting interpolation is used to get a map of values across our new grid object for each time period.
```{r}
pre_fire.idw <- idw(corrected_pm~1, pre_fire[!is.na(pre_fire$corrected_pm),], grd)
spplot(pre_fire.idw[,1], main="Pre-Fire Period on 12/30")

during_fire.idw <- idw(corrected_pm~1, during_fire[!is.na(during_fire$corrected_pm),], grd)
spplot(during_fire.idw[,1], main="During Fire on 12/30")

evening_of_fire.idw <- idw(corrected_pm~1, evening_of_fire[!is.na(evening_of_fire$corrected_pm),], grd)
spplot(evening_of_fire.idw[,1], main="After Fire on 12/30")

december31.idw <- idw(corrected_pm~1, december31[!is.na(december31$corrected_pm),], grd)
spplot(december31.idw[,1], main="December 31st")

january1.idw <- idw(corrected_pm~1, january1[!is.na(january1$corrected_pm),], grd)
spplot(january1.idw[,1], main="January 1st")

january2.idw <- idw(corrected_pm~1, january2[!is.na(january2$corrected_pm),], grd)
# spplot(january2.idw[,1], main="January 2nd") -- ERROR: "'breaks' are not unique"
```

Let's make the maps easier to read. We use the EPA AQI mapping standards for these and create plots that have consistent legends.
```{r}
aqi_colors <- c("#00e400",
                "#ffff00",
                "#ff7e00",
                "#ff0000",
                "#8f3f97",
                "#7e0023")

aqi_labels <- c("Good (0-12.0)",
                "Moderate (12.1-35.4)",
                "Unhealthy for Sensitive Groups (35.5-55.4)",
                "Unhealthy (55.5-150.4)",
                "Very Unhealthy (150.5-250.4)",
                "Hazardous (250.5-500.4)")

factorize_aqi <- function (idw) {
  idw$PM2.5 <- cut(idw$var1.pred, breaks=c(0, 12.0, 35.4, 55.4, 150.4, 250.4), na.omit=T)
  return(idw)
}

plot_idw <- function(idw, title) {
  return(ggplot(all_bounds) +
  geom_sf() +
  geom_sf(data=st_as_sf(factorize_aqi(idw)), aes(col=PM2.5), size=2) +
  scale_color_manual(values=aqi_colors, labels=aqi_labels, name="PM2.5 (µg/m^3)") +
  geom_sf(data=marshall_fire$geometry, col="gray20", alpha=0.1) +
  ggtitle(title))
}

plot_idw(pre_fire.idw, "Pre Fire IDW")
plot_idw(during_fire.idw, "During Fire IDW")
plot_idw(evening_of_fire.idw, "Evening of fire IDW")
plot_idw(december31.idw, "December 31st IDW")
plot_idw(january1.idw, "January 1st IDW")
plot_idw(january2.idw, "January 2nd IDW")
```

Now we take the above plots & plot them as rasters to clear up any visual confusion.
```{r}
r_grd <- raster(grd, nrows=50, ncols=49) # creating a 50x50 object gives some weird visuals

plot_idw_raster <- function(idw, title) {
  r <- raster::rasterize(idw, r_grd, field="var1.pred")
  extent(r) <- st_bbox(all_bounds)
  r_df <- r %>%
    rasterToPoints() %>%
    data.frame() %>%
    mutate(cuts = cut(layer, breaks=c(0, 12.0, 35.4, 55.4, 150.4, 250.4), na.omit=T))
  
  return(ggplot() +
          geom_tile(data=r_df, aes(x=x, y=y, fill=cuts))+
          scale_fill_manual(values=aqi_colors, labels=aqi_labels, name="PM2.5 (µg/m^3)", drop=FALSE) +
          geom_sf(data=marshall_fire$geometry, col="gray20", alpha=0.1) +
          ggtitle(title))
}

# plot_idw_raster(pre_fire.idw, "Pre Fire IDW")
# plot_idw_raster(during_fire.idw, "During Fire IDW")
# plot_idw_raster(evening_of_fire.idw, "Evening of fire IDW")
# plot_idw_raster(december31.idw, "December 31st IDW")
# plot_idw_raster(january1.idw, "January 1st IDW")
# plot_idw_raster(january2.idw, "January 2nd IDW")
```

### Export smoke affected area
```{r}
r <- raster::rasterize(during_fire.idw, r_grd, field="var1.pred")
extent(r) <- st_bbox(all_bounds)
r[r < 12] <- NA
writeRaster(r, "../GIS_inputs_destruction_fireboundary/smoke_affected.tiff", overwrite=TRUE)
```


### Cross Validation
Using leave-one-out cross validation to get residuals for the sensors for each time period. Each sensor is left out once, then the average residual is calculated from the other 37 times the model is created.
```{r}
pre_fire.idw_cv <- krige.cv(corrected_pm~1, pre_fire[!is.na(pre_fire$corrected_pm),], nfold=38)
pre_fire.idw_sd <- sd(pre_fire.idw_cv$residual)
paste("Pre fire standard deviation:", pre_fire.idw_sd)
spplot(pre_fire.idw_cv, main="Pre-Fire Period on 12/30")
pre_fire.idw_cv$zscore <- pre_fire.idw_cv$residual / pre_fire.idw_sd

during_fire.idw_cv <- krige.cv(corrected_pm~1, during_fire[!is.na(during_fire$corrected_pm),], nfold=38)
during_fire.idw_sd <- sd(during_fire.idw_cv$residual)
paste("During fire standard deviation:", during_fire.idw_sd)
spplot(during_fire.idw_cv, main="During Fire on 12/30")
during_fire.idw_cv$zscore <- during_fire.idw_cv$residual / during_fire.idw_sd

evening_of_fire.idw_cv <- krige.cv(corrected_pm~1, evening_of_fire[!is.na(evening_of_fire$corrected_pm),], nfold=38)
evening_of_fire.idw_sd <- sd(evening_of_fire.idw_cv$residual)
paste("Evening of fire standard deviation:", evening_of_fire.idw_sd)
spplot(evening_of_fire.idw_cv, main="After Fire on 12/30")
evening_of_fire.idw_cv$zscore <- evening_of_fire.idw_cv$residual / evening_of_fire.idw_sd

december31.idw_cv <- krige.cv(corrected_pm~1, december31[!is.na(december31$corrected_pm),], nfold=38)
december31.idw_sd <- sd(december31.idw_cv$residual)
paste("December 31st standard deviation:", december31.idw_sd)
spplot(december31.idw_cv, main="December 31st")
december31.idw_cv$zscore <- december31.idw_cv$residual / december31.idw_sd

january1.idw_cv <- krige.cv(corrected_pm~1, january1[!is.na(january1$corrected_pm),], nfold=38)
january1.idw_sd <- sd(january1.idw_cv$residual)
paste("January 1 standard deviation:", january1.idw_sd)
spplot(january1.idw_cv, main="January 1st")
january1.idw_cv$zscore <- january1.idw_cv$residual / january1.idw_sd

#january2.idw_cv <- krige.cv(corrected_pm~1, january2[!is.na(january2$corrected_pm),], nfold=38) -- ERROR: "must have 'max' > 'min'"
# january2.idw_sd <- sd(january2.idw_cv$residual)
# paste("January 2 standard deviation:", january2.idw_sd)
# spplot(january1.idw_cv, main="January 2nd")
# january2.idw_cv$zscore <- january2.idw_cv$residual / january2.idw_sd
```

Plotting these residuals & normalizing them to z-scores for easier visual comparison.
```{r}
plot_idw_resid <- function(idw.cv, title) {
  return(ggplot(all_bounds) +
  geom_sf(fill="#FbFbFb", color="gray") + 
  geom_sf(data=marshall_fire$geometry, col="red", alpha=0.1) +
  geom_sf(data=st_as_sf(idw.cv), aes(fill=residual), color="black", shape=21, size=2) +
  scale_color_gradient2(aesthetics="fill") +
  ggtitle(title))
}

plot_idw_resid_zscore <- function(idw.cv, title) {
  return(ggplot(all_bounds) +
  geom_sf(fill="#FbFbFb", color="gray") + 
  geom_sf(data=marshall_fire$geometry, col="red", alpha=0.1) +
  geom_sf(data=st_as_sf(idw.cv), aes(fill=zscore), color="black", shape=21, size=2) +
  scale_color_gradient2(aesthetics="fill") +
  ggtitle(title))
}

plot_idw_resid(pre_fire.idw_cv, "Pre-Fire Residuals")
plot_idw_resid(during_fire.idw_cv, "During Fire Residuals")
plot_idw_resid(evening_of_fire.idw_cv, "After Fire Residuals")
plot_idw_resid(december31.idw_cv, "December 31st Residuals")
plot_idw_resid(january1.idw_cv, "January 1st Residuals")
# plot_idw_resid(january2.idw_cv, "January 2nd Residuals") ERROR -- doesn't work, see above

plot_idw_resid_zscore(pre_fire.idw_cv, "Pre-Fire Residual Z-Scores")
plot_idw_resid_zscore(during_fire.idw_cv, "During Fire Residual Z-Scores")
plot_idw_resid_zscore(evening_of_fire.idw_cv, "After Fire Residual Z-Scores")
plot_idw_resid_zscore(december31.idw_cv, "December 31st Residual Z-Scores")
plot_idw_resid_zscore(january1.idw_cv, "January 1st Residual Z-Scores")
# plot_idw_resid_zscore(january2.idw_cv, "January 2nd Residual Z-Scores") ERROR -- doesn't work, see above
```

Trying to plot each map with the residuals next to it, however these just look messy.
```{r}
plot_idw_cv <- function(idw, idw_title, idw.cv) {
  p1 <- plot_idw(idw, idw_title)
  p2 <- plot_idw_resid(idw.cv, "Residuals")
  return(grid.arrange(p1, p2, ncol=2))
}

plot_idw_cv(pre_fire.idw, "Pre-Fire IDW", pre_fire.idw_cv)
plot_idw_cv(during_fire.idw, "During Fire IDW", during_fire.idw_cv)
plot_idw_cv(evening_of_fire.idw, "Evening of Fire IDW", evening_of_fire.idw_cv)
plot_idw_cv(december31.idw, "December 31st IDW", december31.idw_cv)
plot_idw_cv(january1.idw, "January 1st IDW", january1.idw_cv)
# plot_idw_cv(january2.idw, "January 2nd IDW", january2.idw_cv) -- ERROR: doesn't work, see above
```


### Notes:

* Pre-fire period the residuals look fairly good, some over & underprediction in downtown Boulder
* During fire period has 1 large underprediction value, few other overpredicted near fire area
* Evening after the fire has underprediction, but not much
* December 31st has high under & over predicted values right next to each other near fire
* Same as December 31st on Jan 1st
* Same as prior 2 days
  + Something happened starting on December 31st with these two sensors? Will look into both and see how the values differ over time
```{r}
# # One sensor is "Eisenhower Dr"
# plot(fortify(PA_STFDF[PA_STFDF@data$Name == "Eisenhower Dr"]["2021-12-30::2022-01-02", "corrected_pm"])) -- ERROR: "error in evaluating the argument 'x' in selecting a method for function 'plot': missing value where TRUE/FALSE needed"
# # The other is "Purple House"
# plot(fortify(PA_STFDF[PA_STFDF@data$Name == "Purple House"]["2021-12-30::2022-01-02", "corrected_pm"]), col="red") -- ERROR: "error in evaluating the argument 'x' in selecting a method for function 'plot': missing value where TRUE/FALSE needed"

```

The issue is coming from the Eisenhower Dr sensor most likely, as this is the one that seems messed up by the snow. Going to see how everything looks excluding the high values for this sensor. This fed back into our analysis, and has already been accounted for in the current models.

```{r}
# plot(fortify(PA_STFDF[PA_STFDF@data$Name == "Eisenhower Dr"]["2021-12-30::2022-01-02", "corrected_pm"]) %>%
#                filter(corrected_pm <= 1000)) -- ERROR: "error in evaluating the argument 'x' in selecting a method for function 'plot': missing value where TRUE/FALSE needed"
```

Checking the sensor on the edge of the fire boundary with high residuals/z scores.
```{r}
# plot(fortify(PA_STFDF[PA_STFDF@data$Name == "ntsky"]["2021-12-30::2022-01-02", "corrected_pm"])) -- ERROR: "error in evaluating the argument 'x' in selecting a method for function 'plot': missing value where TRUE/FALSE needed"

```


### Looping at Hourly Timescale
We create gifs to make visualization easier, as well as looking at hourly timescales instead of the six predefined fire periods.
```{r, animation.hook="gifski", interval=1.5, results='hide'}
for (j in c(30, 31, 01, 02)) {
  if (j > 3) {
    date <- paste("2021-12-", j, sep="")
  } else {
    date <- paste("2022-01-", j, sep="")
  }
  for (i in 0:23) {
    if (i < 10) {
      i <- paste("0", i, sep="")
    }
    t <- paste(date, " ", i, ":00:00", sep="")
    i.data <- krig_data[, t]
    i.idw <- idw(corrected_pm~1, i.data[!is.na(i.data$corrected_pm),], grd)
    print(plot_idw_raster(i.idw, paste(t, "IDW")))
  }
}
```

```{r, animation.hook='gifski', interval=1.5, results='hide'}
# ERROR: "must have 'max' > 'min'"; note that many of the cv plots did run before the error was thrown
# for (j in c(30, 31, 01, 02)) {
#   if (j > 3) {
#     date <- paste("2021-12-", j, sep="")
#   } else {
#     date <- paste("2022-01-", j, sep="")
#   }
#   for (i in 0:23) {
#     if (i < 10) {
#       i <- paste("0", i, sep="")
#     }
#     t <- paste(date, " ", i, ":00:00", sep="")
#     i.data <- krig_data[, t]
#     i.idw <- idw(corrected_pm~1, i.data[!is.na(i.data$corrected_pm),], grd)
#     i.idw_cv <- krige.cv(corrected_pm~1, i.data[!is.na(i.data$corrected_pm),], nfold=38)
#     i.idw_sd <- sd(i.idw_cv$residual)
#     i.idw_cv$zscore <- i.idw_cv$residual / i.idw_sd
#     print(plot_idw_resid_zscore(i.idw_cv, paste(t, "IDW Residual Z-Scores")))
#   }
# }
```

# gif
```{r, animation.hook="gifski", interval=1.5, results='hide'}
# jan 1 - feb 4th inclusive
for (j in 1:4) {
  date <- paste("2022-0", j, sep="")
  if (j %in% c(1, 3, 5)) {
    range = 1:31
  } else if (j == 2) {
    range = 1:28
  } else {
    range = 1:30
  }
  
  for (i in range) {
    if (i < 10) {
      i <- paste("0", i, sep="")
    }
    day <- paste(date, "-", i, sep="")
    
    data <- daily[, day]
    idw <- idw(corrected_pm~1, data[!is.na(data$corrected_pm),], grd)
    print(plot_idw_raster(idw, paste(day, "IDW")))
    }
}
```

### Weird Sensor from Gif
This sensor seemed to be really skewing our results starting in April 2022. Looking at the purple air website, this sensor has 0% confidence now, with values looking to lose credibility around the start of April, so earlier all of those values were dropped. (this already fed back into the current analysis we're looking at).
```{r}
plot(daily[, "2022-04-25"]$corrected_pm) # sensor at index 29 is weird = sensor ID 102614
plot(fortify(PA_STFDF[PA_STFDF@data$ID == 102614]['2022-04', "corrected_pm"]))
# ponderosa sensor has 0% confidence on purpleair map at present date
```
