---
title: "purple_air"
author: "Emma Rieves"
date: "2/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(readr)
library(plyr)
library(sf)
library(raster)
library(rgdal)
library(ggplot2)
library(spacetime)
library(gridExtra)
library(stringr)
library(leaflet)
library(tidygeocoder)

# PM2.5 package recommended by Priyanka
library(bjzresc)

# to make the spherical geometry errors go away
sf::sf_use_s2(FALSE)
```

# Download municipal boundary data

```{r}
# IF USING THE MARSHALL FIRE BOUNDARY TO EXTRACT STATIONS:

## Use marshall fire boundary to extract stations
#wfigs_fire = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/WFIGS_-_Wildland_Fire_Perimeters_Full_History/FH_Perimeter.shp")

#(marshall_fire = wfigs_fire %>% filter(poly_Incid == "Marshall"))

#(prg = crs(marshall_fire,asText=TRUE))
```

```{r}
# IF USING BOULDER COUNTIES TO EXTRACT STATIONS

## Use boulder county shapefile to extract municipal boundaries for Boulder counties
BO_CO = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/Boulder_county_munis/Municipalities.shp")

(fire_counties = BO_CO %>% filter(ZONEDESC == "Louisville" | ZONEDESC == "Superior" | ZONEDESC == "Broomfield" | ZONEDESC == "Lafayette" | ZONEDESC == "Boulder"))

prg = raster::crs(fire_counties,asText=TRUE)

```

# Download AQ data 
```{r}
## use bjzresc package to get list of purple air sensors; save to df instead of csv
pa_download = getPurpleairLst(output.path = NULL)
```

```{r}
## Intersect municipal boundaries with PA sensors

# remove null Lat/Long PA sensors -- important to creating spatial dataframe
pa_download = pa_download[complete.cases(pa_download[c("Lat","Lon")]),]

# check that it worked
sum(is.na(pa_download[c("Lat","Lon")]))

# create spatial dataframe, set CRS to match muni boundaries
pa_download_spatial = pa_download %>% 
  st_as_sf(coords = c("Lon","Lat")) %>% 
  st_set_crs(prg)

# check CRS -- yes
raster::crs(pa_download_spatial)

# intersect PA download area and fire affected area to download sensors
fire_affected_sensors = st_intersection(pa_download_spatial,st_buffer(fire_counties,0))
#fire_affected_sensors2 = st_intersection(pa_download_spatial,st_buffer(marshall_fire,0))

# get sensor IDs for sensors in fire affected area
fire_area_sensor_IDs = fire_affected_sensors$ID
#fire_area_sensor_IDs2 = fire_affected_sensors2$ID

# filter original dataframe to include only sensors in fire affected area
(fire_area_sensors = pa_download[pa_download$ID %in% fire_area_sensor_IDs, ])
#(fire_area_sensors2 = pa_download[pa_download$ID %in% fire_area_sensor_IDs2, ])

```


```{r eval=F}
# download purple air data -- TAKES A LONG TIME TO RUN SO BE READY FOR THAT
## output path is a folder that stores a csv for each sensor for the target time period
## average means that data is averaged for 10-minute intervals
## indoor = TRUE includes indoor sensor observations

purpleairDownload(site.csv = fire_area_sensors, start.date = "2021-12-30", end.date = "2022-05-01", output.path = "/Users/esrieves/Documents/GitHub/marshall_fires/fire_counties_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
```

```{r}
# download purple air data (with above specifications) for the Marshall fire boundary
#purpleairDownload(site.csv = fire_area_sensors2, start.date = "2021-12-30", end.date = "2022-04-27", output.path = "marshall_fire_path_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
```


# Process and clean data

## Helper function -- OLD
```{r}
# OLD HELPER FUNCTION USED TO DETERMINE LOCATION
# get_sensor_info <- function(id) {
#   # for each id, get the rest of the information (including lat/long)
#   info = id_key[id_key$ID == id,]
#   # create a spatial version of info
#   info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
#   #intersect ID info with the county map to determine location
#   city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
#   city = city_intersect$ZONEDESC
#   # return dataframe with: ID, lat/long, indoor/outdoor, and city
#   info = info %>%
#     mutate(city = ifelse(length(city) == 0, NA, city)) %>%
#     dplyr::select(-Name)
#   return(city)
# }

# create a key coordinating the sensor ID number to its lat/lon
(id_key = AQ_df %>%
  group_by(ID) %>%
  dplyr::select(ID, Lon, Lat, Name, Location) %>%
  unique() %>%
  # remove NAs for spatial intersections to occur
  na.omit()
  )

# id_key %>% rowwise() %>% mutate(place = get_sensor_info(ID))
# 
# 
# # use helper function to get municipality for each sensor
# sensor_info = get_sensor_info(id_key$ID)
# 
# sensor_info %>% group_by(city) %>% dplyr::summarise(n = n())
```


## Combine & clean data
```{r}
## Read downloaded PA files from their filepath and turn them into a DF

# directory where files are stored
dir = "/Users/esrieves/Documents/GitHub/marshall_fires/fire_counties_PAs/"

# create a list of all file names in this directory
file_name = list.files(path=dir, pattern="*.csv", full.names=TRUE)

# read csvs for each filename in list --> results in a list of lists
AQ_files = lapply(file_name, read_csv)

# combine all AQ lists from each directory into AQ dataframe 
(AQ_df = rbind.fill(AQ_files) %>% as.data.frame())
```

```{r}
# create a date column in POSIX format to create time series
AQ_df$datetime = as.POSIXct(AQ_df$created_at)

# rename and select important columns 
AQ_df = AQ_df %>% 
  dplyr::rename(pm25_a = `PM2.5_CF_ATM_ug/m3_A`,
                 pm25_b = `PM2.5_CF_ATM_ug/m3_B`,
                 temp = Temperature_F_A,
                 rh = `Humidity_%_A`)

# create hourly pm column
AQ_df$hour = as.POSIXlt(AQ_df$datetime)$hour

# Convert numeric values to a numeric class
AQ_df$pm25_a = as.numeric(AQ_df$pm25_a)
AQ_df$pm25_b = as.numeric(AQ_df$pm25_b)
AQ_df$temp = as.numeric(AQ_df$temp)
AQ_df$rh = as.numeric(AQ_df$rh)
```


```{r}
# fill in time series
(AQ_df = AQ_df %>%
  dplyr::select(ID, Name, Lon, Lat, Location, datetime, pm25_a, pm25_b, temp, rh) %>%
  group_by(ID) %>%
  # add in NAs to timeseries to calculate % complete (before missing time periods were just absent and not NAs in the dataset)
  complete(datetime = seq(min(datetime), max(datetime), by = "10 min")))

```
## geocode
```{r}
# GEOCODE
# takes a while to run.. uses lat/long to "reverse" geocode (with tidygeocoder package) and provide address (including city/zip)
sensor_info = reverse_geocode(id_key,lat=Lat,long = Lon)

# clean address to extract zip code and city name (city update)
# reviewing this shows that some cities didn't turn out correctly.. I just looked up addresses
##### Rock Creek Ranch II -- Superior
##### Broadway -- Boulder (address on Broadway)
sensor_info = sensor_info %>% mutate(address_split = sub(", United States.*","",address),
                        zip_code = str_extract(address_split, "\\w+$"),
                        address_split2 = sub(", Boulder County.*","",address),
                        city = str_extract(address_split2, "\\w+$"),
                        # recode cities that didn't show up properly
                        city_update = ifelse(city == "II", "Superior",
                                             ifelse(city == "Broadway", "Boulder",city))) %>%
  dplyr::select(-c(address_split,address_split2,city))

# check distribution of sensors
sensor_info %>% group_by(city_update) %>% dplyr::summarise(n = n())
```
## add in classifications for "fire classification" and data completeness by fire period and month
```{r}
## NEED TO RECLASSIFY FIRE PERIODS

(time_period_classification = AQ_df %>%
  mutate(time_period = ifelse(datetime<as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")), "pre_fire_period",
                              ifelse(datetime>=as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")) & datetime<= as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "fire_period",
                                     ifelse(datetime>as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "post_fire_period", "other time")))) %>%
  group_by(ID,time_period) %>%
  dplyr::summarize(
    complete_a= sum(complete.cases(pm25_a))/n()*100
    #complete_b = sum(complete.cases(pm25_b))/n()*100
  ) %>%
  pivot_wider(names_from = time_period, values_from = complete_a) %>%
  rowwise() %>%
  mutate(Status = case_when(
    fire_period >= 75 & post_fire_period >= 85 & pre_fire_period >= 95 ~ "Complete data throughout fire period",
    fire_period < 75 & post_fire_period >= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, returned online",
    fire_period < 75 & post_fire_period <= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, did not return online",
    fire_period < 75 & post_fire_period <= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, did not return online",
    fire_period < 75 & post_fire_period >= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, returned online",
    is.na(fire_period) & is.na(pre_fire_period) ~ "Sensor came online after fire"
  )))

(month_added = AQ_df %>%
  group_by(ID) %>%
  dplyr::select(ID, datetime) %>%
  dplyr::summarize(Month = ifelse(format(datetime[1], "%m-%Y") == "12-2021","Before or during 12-2021",format(datetime[1], "%m-%Y"))))

# merge time period classification with month added, then merge with the sensor info (contains address, zip, city)
sensor_data = merge(time_period_classification,month_added,by="ID")
(sensor_data_full = merge(sensor_data,sensor_info,by="ID"))


## factor to order time periods and status
sensor_data_full$Month = factor(sensor_data_full$Month, levels = c("Before or during 12-2021","01-2022","02-2022","03-2022","04-2022"))
sensor_data_full$Status = factor(sensor_data_full$Status, levels = c("Complete data throughout fire period", "Sensor offline during fire, returned online", "Sensor offline during fire, did not return online", "Sensor added during fire, returned online", "Sensor added during fire, did not return online", "Sensor came online after fire"))

```


# Export data
## Data for AQ cleaning and analysis
```{r}
# AQ df for correction script
write.csv(AQ_df,"/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
```

## Sensor information for mapping
```{r}
# sensor info for visualization script
write.csv(sensor_data_full,"/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/sensor_data_full.csv")
```


