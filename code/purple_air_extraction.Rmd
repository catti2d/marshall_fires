---
title: "purple_air"
author: "Emma Rieves"
date: "2/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(readr)
library(plyr)
library(sf)
library(raster)
library(rgdal)
library(ggplot2)
library(spacetime)
library(gridExtra)
library(stringr)
library(leaflet)

# PM2.5 package recommended by Priyanka
library(bjzresc)

# to make the spherical geometry errors go away
sf::sf_use_s2(FALSE)
```

# Load data
```{r}
## use bjzresc package to get list of purple air sensors; save to df instead of csv
pa_download = getPurpleairLst(output.path = NULL)
```

```{r}
## Use marshall fire boundary to extract stations
wfigs_fire = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/WFIGS_-_Wildland_Fire_Perimeters_Full_History/FH_Perimeter.shp")

(marshall_fire = wfigs_fire %>% filter(poly_Incid == "Marshall"))

(prg = crs(marshall_fire,asText=TRUE))
```

```{r}
## Use boulder county shapefile to extract municipal boundaries for Louisville and Superior
BO_CO = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/Boulder_county_munis/Municipalities.shp")

(fire_counties = BO_CO %>% filter(ZONEDESC == "Louisville" | ZONEDESC == "Superior" | ZONEDESC == "Broomfield" | ZONEDESC == "Lafayette"))

# same as marshall fire
(crs(fire_counties,asText=TRUE))
```

```{r}
## Read in destroyed home and building sites
destroyed_homes = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_homes.shp")
damaged_homes = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/damaged_homes.shp")
destroyed_businesses = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/destroyed_businesses.shp")
damaged_businesses = st_read("/Users/esrieves/Documents/GitHub/marshall_fires/GIS_inputs_destruction_fireboundary/output_damage_files/damaged_businesses.shp")

# check that crs matches leaflet
(crs(destroyed_homes))

# mutate type and select columns needed
(destroyed_homes = destroyed_homes %>% mutate(type = "destroyed - residential") %>% dplyr::rename(jurisdiction = JURISDICTI) %>% dplyr::select(jurisdiction, type, latlong, geometry))
(destroyed_businesses = destroyed_businesses  %>% mutate(type = "destroyed - non-residential") %>% dplyr::rename(jurisdiction = Jurisdicti) %>% dplyr::select(jurisdiction, type, latlong, geometry))
(damaged_homes = damaged_homes %>% mutate(type = "damaged - residential") %>% dplyr::rename(jurisdiction = JURISDICTI) %>% dplyr::select(jurisdiction, type, latlong, geometry))
(damaged_business = damaged_businesses %>% mutate(type = "damaged - non-residential") %>% dplyr::rename(jurisdiction = Jurisdicti) %>% dplyr::select(jurisdiction, type, latlong, geometry))

# combine into one df, using the "," indicator keeps decimal points
(destroyed_damaged =rbind(destroyed_homes,destroyed_businesses,damaged_homes,damaged_business) %>% separate(latlong, c("lat","long"), ",", convert = FALSE))

# change lat and long to floats
destroyed_damaged$lat = as.double(destroyed_damaged$lat)
destroyed_damaged$long = as.double(destroyed_damaged$long)
```


# Download AQ data 
```{r}
## Intersect municipal boundaries with PA sensors

# remove null Lat/Long PA sensors -- important to creating spatial dataframe
pa_download = pa_download[complete.cases(pa_download[c("Lat","Lon")]),]

# check that it worked
sum(is.na(pa_download[c("Lat","Lon")]))

# create spatial dataframe 
pa_download_spatial = pa_download %>% 
  st_as_sf(coords = c("Lon","Lat")) %>% 
  st_set_crs(prg)

# check CRS
crs(pa_download_spatial)

# intersect PA download area and fire affected area to download sensors
fire_affected_sensors = st_intersection(pa_download_spatial,st_buffer(fire_counties,0))
fire_affected_sensors2 = st_intersection(pa_download_spatial,st_buffer(marshall_fire,0))

# get sensor IDs for sensors in fire affected area
fire_area_sensor_IDs = fire_affected_sensors$ID
fire_area_sensor_IDs2 = fire_affected_sensors2$ID

# filter original dataframe to include only sensors in fire affected area
(fire_area_sensors = pa_download[pa_download$ID %in% fire_area_sensor_IDs, ])
(fire_area_sensors2 = pa_download[pa_download$ID %in% fire_area_sensor_IDs2, ])

```


```{r eval=F}
# download purple air data -- TAKES A LONG TIME TO RUN SO BE READY FOR THAT
## output path is a folder that stores a csv for each sensor for the target time period
## average means that data is averaged for 10-minute intervals
## indoor = TRUE includes indoor sensor observations

purpleairDownload(site.csv = fire_area_sensors, start.date = "2021-12-30", end.date = "2022-04-27", output.path = "fire_counties_PAs2/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
```

```{r}
purpleairDownload(site.csv = fire_area_sensors2, start.date = "2021-12-30", end.date = "2022-04-27", output.path = "marshall_fire_path_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
```


# Process and clean data

## Helper function -- provides information about sensor
```{r}
get_sensor_info <- function(ID) {
  # for each id, get the rest of the information (including lat/loing)
  info = id_key[id_key$ID == ID,]
  # create a spatial version of info
  info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
  # intersect ID info with the county map to determine location
  city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
  city = city_intersect$ZONEDESC
  # return dataframe with: ID, lat/long, indoor/outdoor, and city
  info = info %>%
    mutate(city = ifelse(length(city) == 0, NA, city)) %>%
    dplyr::select(-Name)
  return(info)
}
```


## Combine & clean data
```{r}
## Read downloaded PA files from their filepath and turn them into a DF

# directory where files are stored
dir = "fire_counties_PAs/"
dir2 = "marshall_fire_path_PAs/"

# create a list of all file names in this directory
file_name1 = list.files(path=dir, pattern="*.csv", full.names=TRUE)
file_names2 = list.files(path=dir2, pattern="*.csv", full.names=TRUE)

# read csvs for each filename in list --> results in a list of lists
AQ_files1 = lapply(file_name1, read_csv)
AQ_files2 = lapply(file_names2, read_csv)

```

```{r}
## Combine lists of sensor data into a single dataframe

# combine all AQ lists from each directory into single AQ dataframe for each directory
(AQ_df1 = rbind.fill(AQ_files1) %>% as.data.frame())
(AQ_df2 = rbind.fill(AQ_files2) %>% as.data.frame())

# combine marshall fire and superior/lousiville AQ sites into a single df
(AQ_df = rbind(AQ_df1, AQ_df2))
```

```{r}
# create a date column in POSIX format to create time series
AQ_df$datetime = as.POSIXct(AQ_df$created_at)

# rename and select important columns 
AQ_df = AQ_df %>% 
  dplyr::rename(pm25_a = `PM2.5_CF_1_ug/m3_A`,
                 pm25_b = `PM2.5_CF_1_ug/m3_B`,
                 temp = Temperature_F_A,
                 rh = `Humidity_%_A`)

# create hourly pm column
AQ_df$hour = as.POSIXlt(AQ_df$datetime)$hour

# Convert numeric values to a numeric class
AQ_df$pm25_a = as.numeric(AQ_df$pm25_a)
AQ_df$pm25_b = as.numeric(AQ_df$pm25_b)
AQ_df$temp = as.numeric(AQ_df$temp)
AQ_df$rh = as.numeric(AQ_df$rh)
```


```{r}
# fill in time series

## complete missing parts of the time series 
(AQ_df = AQ_df %>%
  dplyr::select(ID, Name, Lon, Lat, Location, datetime, pm25_a, pm25_b, temp, rh) %>%
  group_by(ID) %>%
  # add in NAs to timeseries to calculate % complete (before missing time periods were just absent and not NAs in the dataset)
  complete(datetime = seq(min(datetime), max(datetime), by = "10 min")))

```

```{r}
# add new information: time period classification, completeness

## add time period classification: "pre-fire", "during fire", "post fire"
(time_period_classification = AQ_df %>%
  mutate(time_period = ifelse(datetime<as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")), "pre_fire_period",
                              ifelse(datetime>=as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")) & datetime<= as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "fire_period",
                                     ifelse(datetime>as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "post_fire_period", "other time")))))

## factor to order time periods
time_period_classification$time_period = factor(time_period_classification$time_period, levels = c("pre_fire_period","fire_period","post_fire_period","other time"))

## add in data completeness: "complete data", "sensor offline during fire did/did not return online", "sensor adding during fire did/did not return online", "sensor added after fire"
(time_period_classification = time_period_classification %>%
  group_by(ID,time_period) %>%
  dplyr::summarize(
    complete_a= sum(complete.cases(pm25_a))/n()*100
    #complete_b = sum(complete.cases(pm25_b))/n()*100
  ) %>%
  pivot_wider(names_from = time_period, values_from = complete_a) %>%
  rowwise() %>%
  mutate(Status = case_when(
    fire_period >= 75 & post_fire_period >= 85 & pre_fire_period >= 95 ~ "Complete data throughout fire period",
    fire_period < 75 & post_fire_period >= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, returned online",
    fire_period < 75 & post_fire_period <= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, did not return online",
    fire_period < 75 & post_fire_period <= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, did not return online",
    fire_period < 75 & post_fire_period >= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, returned online",
    is.na(fire_period) & is.na(pre_fire_period) ~ "Sensor came online after fire"
  )))

## factorize the status so it shows up better in maps
time_period_classification$Status = factor(time_period_classification$Status, levels = c("Complete data throughout fire period", "Sensor offline during fire, returned online", "Sensor offline during fire, did not return online", "Sensor added during fire, returned online", "Sensor added during fire, did not return online", "Sensor came online after fire"))
time_period_classification
```

```{r}
# create a key coordinating the sensor ID number to its lat/lon
(id_key = AQ_df %>%
  group_by(ID) %>%
  dplyr::select(ID, Lon, Lat, Name, Location) %>%
  unique() %>%
  # remove NAs for spatial intersections to occur
  na.omit()
  )

# use helper function to get municipality for each sensor
sensor_info = get_sensor_info(id_key$ID)
```

```{r}
# combine fire period classification and ID data into sensor stats
sensor_stats = cbind(sensor_info, time_period_classification, by = "ID")

# remove additional columns created in the merge and rename duplicated columns (ID)
sensor_stats = sensor_stats %>% 
  dplyr::select(-`ID...6`,-by) %>%
  dplyr::rename(ID = `ID...1`)
```

# Export data
## Data for AQ cleaning and analysis
```{r}
# AQ df for cleaning
write.csv(AQ_df,"/Users/esrieves/Documents/GitHub/marshall_fires/intermediary_outputs/aq_data.csv")
```

## Sensor information for mapping
```{r}

```


